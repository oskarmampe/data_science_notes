\documentclass{journal}

\usepackage{parskip}

\setlength{\parindent}{0cm}

\begin{document}

\section{Introduction to Statistics}
You're a citizen scientist who has started collecting data about rising water in the river next to where you live. For months, you painstakingly measure the water levels and enter your findings into a notebook. But at the end of it, what exactly do you have? What can all this data tell us?

In this lesson, we'll explore how we can use NumPy to analyze data. We'll learn different methods to calculate common statistical properties of a dataset, such as finding the mean and standard deviation. By the end, you'll be able to do basic analysis of a dataset and understand how we can use statistics to come to conclusions about data.

The statistical concepts that we'll cover include:

\begin{itemize}
	\item Mean
	\item Median
	\item Percentiles
	\item Interquartile Range
	\item Outliers
	\item Standard Deviation
	
\end{itemize}

To start, we'll be analyzing single-variable datasets. One way to think of a single-variable dataset is that it contains answers to a question. For instance, we might ask 100 people, “How tall are you?” Their heights in inches would form our dataset.

For our purposes, we'll be organizing our datasets into NumPy arrays. To learn more about NumPy arrays, take our course Learn NumPy: Introduction.

\subsection{Code}

Introduction to Statistics with NumPy

After the river in your town flooded during a recent hurricane, you've become interested in collecting data about the its height. Every day for the past month, you walk to the river, measure the height of the water, and enter this information into a notebook.

Let's look at how you can use NumPy functions to analyze your dataset.

First, we'll import the NumPy module, so we can use its statistical calculation functions.

import numpy as np

water_height = np.array([4.01, 4.03, 4.27, 4.29, 4.19,
                         4.15, 4.16, 4.23, 4.29, 4.19,
                         4.00, 4.22, 4.25, 4.19, 4.10,
                         4.14, 4.03, 4.23, 4.08, 14.20,
                         14.03, 11.20, 8.19, 6.18, 4.04,
                         4.08, 4.11, 4.23, 3.99, 4.23])

Let's use the function np.mean() to find the average water height:

np.mean(water_height)

5.2510000000000003

But wait! We should sort our data to see if there could be any measurements to throw our data off, or represent a deviation from the mean:

np.sort(water_height)

array([  3.99,   4.  ,   4.01,   4.03,   4.03,   4.04,   4.08,   4.08,
         4.1 ,   4.11,   4.14,   4.15,   4.16,   4.19,   4.19,   4.19,
         4.22,   4.23,   4.23,   4.23,   4.23,   4.25,   4.27,   4.29,
         4.29,   6.18,   8.19,  11.2 ,  14.03,  14.2 ])

Looks like that thunderstorm might have impacted the average height! Let's measure the median to see if its more representative of the dataset:

np.median(water_height)

4.1900000000000004

While the median tells us where half of our data lies, let's look at a value closer to the end of the dataset. We can use percentiles to use a data points position and get its value:

np.percentile(water_height, 75)

4.2649999999999997

So far, we've gotten a good idea about specific values. But what about the spread of our data? Let's calculate the standard deviation to understand how similar or how different each data point is:

np.std(water_height)

2.784585367099861

Great! Just using a few simple functions we've been able to quickly calculate several important measurements and can begin analyzing our dataset.


\section{Mean}

The first statistical concept we'll explore is mean, also commonly referred to as an average. The mean is a useful measurement to get the center of a dataset. NumPy has a built-in function to calculate the average or mean of arrays: np.mean

Let's say we want to find the average number of pounds of produce a person purchases per week. We administered a survey and received 1,000 responses:

survey_responses = [5, 10.2, 4, .3 ... 6.6]

We can then transform the dataset into a NumPy array and use the function np.mean to calculate the average:

>>> survey_array = np.array(survey_responses)
>>> np.mean(survey_array)
5.220

\subsection{Mean with NumPy}
We can also use np.mean to calculate the percent of array elements that have a certain property.

As we know, a logical operator will evaluate each item in an array to see if it matches the specified condition. If the item matches the given condition, the item will evaluate as True and equal 1. If it does not match, it will be False and equal 0.

When np.mean calculates a logical statement, the resulting mean value will be equivalent to the total number of True items divided by the total array length.

In our produce survey example, we can use this calculation to find out the percentage of people who bought more than 8 pounds of produce each week:

>>> np.mean(survey_array > 8)
0.2

The logical statement survey_array > 8 evaluates which survey answers were greater than 8, and assigns them a value of 1. np.mean adds all of the 1s up and divides them by the length of survey_array. The resulting output tells us that 20% of responders purchased more than 8 pounds of produce.

\subsection{Mean with Multiple Dimension Arrays}
Calculating the Mean of 2D Arrays

If we have a two-dimensional array, np.mean can calculate the means of the larger array as well as the interior values.

Let's imagine a game of ring toss at a carnival. In this game, you have three different chances to get all three rings onto a stick. In our ring_toss array, each interior array (the arrays within the larger array) is one try, and each number is one ring toss. 1 represents a successful toss, 0 represents a fail.

First, we can use np.mean to find the mean across all the arrays:

>>> ring_toss = np.array([[1, 0, 0], 
                          [0, 0, 1], 
                          [1, 0, 1]])
>>> np.mean(ring_toss)
0.44444444444444442

To find the means of each interior array, we specify axis 1 (the "rows"):

>>> np.mean(ring_toss, axis=1)
array([ 0.33333333,  0.33333333,  0.66666667])

To find the means of each index position (i.e, mean of all 1st tosses, mean of all 2nd tosses, ...), we specify axis 0 (the "columns"):

>>> np.mean(ring_toss, axis=0)
array([ 0.66666667,  0.        ,  0.66666667])


\section{Outliers}
As we can see, the mean is a helpful way to quickly understand different parts of our data. However, the mean is highly influenced by the specific values in our data set. What happens when one of those values is significantly different from the rest?

Values that don’t fit within the majority of a dataset are known as outliers. It’s important to identify outliers because if they go unnoticed, they can skew our data and lead to error in our analysis (like determining the mean). They can also be useful in pointing out errors in our data collection.

When we're able to identify outliers, we can then determine if they were due to an error in sample collection or whether or not they represent a significant but real deviation from the mean.

Suppose we want to determine the average height for 3rd graders. We measure several students at the local school, but accidentally measure one student in centimeters rather than in inches. If we're not paying attention, our dataset could end up looking like this:

[50, 50, 51, 49, 48, 127]

In this case, 127 would be an outlier.

Some outliers aren’t the result of a mistake. For instance, suppose that one of our 3rd graders had skipped a grade and was actually a year younger than everyone else in the class:

[50, 50, 51, 49, 48, 45]

She might be significantly shorter at 45", but her height would still be an outlier.

Suppose that another student was just unusually tall for his age:

[50, 50, 51, 49, 48, 58.5]

His height of 58.5" would also be an outlier.

\subsection{Sorting and Outliers}
Sorting and Outliers

One way to quickly identify outliers is by sorting our data, Once our data is sorted, we can quickly glance at the beginning or end of an array to see if some values lie far beyond the expected range. We can use the NumPy function np.sort to sort our data.

Let’s go back to our 3rd grade height example, and imagine an 8th grader walked into our experiement:

>>> heights = np.array([49.7, 46.9, 62, 47.2, 47, 48.3, 48.7])

If we use np.sort, we can immediately identify the taller student since their height (62") is noticeably outside the range of the dataset:

>>> np.sort(heights)
array([ 46.9,  47. ,  47.2,  48.3,  48.7,  49.7,  62])

\section{Median}
Another key metric that we can use in data analysis is the median. The median is the middle value of a dataset that’s been ordered in terms of magnitude (from lowest to highest).

Let's look at the following array:

np.array( [1, 1, 2, 3, 4, 5, 5])

In this example, the median would be 3, because it is positioned half-way between the minimum value and the maximum value.

If the length of our dataset was an even number, the median would be the value halfway between the two central values. So in the following example, the median would be 3.5:

np.array( [1, 1, 2, 3, 4, 5, 5, 6])

But what if we had a very large dataset? It would get very tedious to count all of the values. Luckily, NumPy also has a function to calculate the median, np.median:

>>> my_array = np.array([50, 38, 291, 59, 14])
>>> np.median(my_array)
50.0

\section{Mean vs Median}
In a dataset, the median value can provide an important comparison to the mean. Unlike a mean, the median is not affected by outliers. This becomes important in skewed datasets, datasets whose values are not distributed evenly. 

\section{Percentiles}
As we know, the median is the middle of a dataset: it is the number for which 50% of the samples are below, and 50% of the samples are above. But what if we wanted to find a point at which 40% of the samples are below, and 60% of the samples are above?

This type of point is called a percentile. The Nth percentile is defined as the point N% of samples lie below it. So the point where 40% of samples are below is called the 40th percentile. Percentiles are useful measurements because they can tell us where a particular value is situated within the greater dataset.

Let's look at the following array:

d = [1, 2, 3, 4, 4, 4, 6, 6, 7, 8, 8]

There are 11 numbers in the dataset. The 40th percentile will have 40% of the 10 remaining numbers below it (40% of 10 is 4) and 60% of the numbers above it (60% of 10 is 6). So in this example, the 40th percentile is 4.

In NumPy, we can calculate percentiles using the function np.percentile, which takes two arguments: the array and the percentile to calculate.

Here's how we would use NumPy to calculate the 40th percentile of array d:

>>> d = np.array([1, 2, 3, 4, 4, 4, 6, 6, 7,  8, 8])
>>> np.percentile(d, 40)
4.00

Some percentiles have specific names:

    The 25th percentile is called the first quartile
    The 50th percentile is called the median
    The 75th percentile is called the third quartile

The minimum, first quartile, median, third quartile, and maximum of a dataset are called a five-number summary. This set of numbers is a great thing to compute when we get a new dataset.

The difference between the first and third quartile is a value called the interquartile range. For example, say we have the following array:

d = [1, 2, 3, 4, 4, 4, 6, 6, 7, 8, 8]

We can calculate the 25th and 75th percentiles using np.percentile:

np.percentile(d, 25)
>>> 3.5
np.percentile(d, 75)
>>> 6.5

Then to find the interquartile range, we subtract the value of the 25th percentile from the value of the 75th:

6.5 - 3.5 = 3

50% of the dataset will lie within the interquartile range. The interquartile range gives us an idea of how spread out our data is. The smaller the interquartile range value, the less variance in our dataset. The greater the value, the larger the variance.


\section{Standard Deviation}
While the mean and median can tell us about the center of our data, they do not reflect the range of the data. That's where standard deviation comes in.

Similar to the interquartile range, the standard deviation tells us the spread of the data. The larger the standard deviation, the more spread out our data is from the center. The smaller the standard deviation, the more the data is clustered around the mean.


\section{Review of NumPy}
Let's review! In this lesson, you learned how to use NumPy to analyze single-variable datasets. Here's what we covered:

    Using the np.sort method to locate outliers.
    Calculating central positions of a dataset using np.mean and np.median.
    Understanding the spread of our data using percentiles and the interquartile range.
    Finding the standard deviation of a dataset using np.std.

In our next lesson, we'll continue our exploration of NumPy and see how we can use it to analyze different statistical distributions. Follow the checkpoints below to practice what you just learned!


\section{Distributions}

A university wants to keep track of the popularity of different programs over time, to ensure that programs are allocated enough space and resources. You work in the admissions office and are asked to put together a set of visuals that show these trends to interested applicants. How can we calculate these distributions? Would we be able to see trends and predict the popularity of certain programs in the future? How would we show this information?

In this lesson, we are going to learn how to use NumPy to analyze different distributions, generate random numbers to produce datasets, and use Matplotlib to visualize our findings.

This lesson will cover:

    How to generate and graph histograms
    How to identify different distributions by their shape
    Normal distributions
    How standard deviations relate to normal distributions
    Binomial distributions



\subsection{Statistical Distributions with NumPy}

Imagine that you work as an admissions officer at a university. Part of your job is to collect, analyze, and visualize data that's relevant to interested applicants.

Recently, you've become interested in how histograms can show different distributions of populations and even occurences. You think that histograms would be useful in visualizing different trends, such as changes in department numbers and participation in extracurriculars. You also want to learn more about how you can use randomly generated distributions to make statistical calculations and predict the probability of future events, such as the sucess of your ultimate frisbee team.

For this lesson, we'll be using NumPy to calculate distributions and Matplotlib to graph our calculations.

import numpy as np
from matplotlib import pyplot as plt

One set of data you want to analyze is enrollment in different degree programs. By looking at histograms of the number of years students are enrolled in a program, you can identify what programs are becoming more popular, which are falling out of favor, and which have steady, continual enrollment.

First, let's look at how many hundreds of students decide to enroll in Codecademy University and how many years they've been enrolled.

total_enrollment = [1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5]

plt.hist(total_enrollment, bins=5, range=(1, 6))
plt.title('Student Enrollment (Codecademy University)')
plt.xlabel('Years Enrolled')
plt.ylabel('Students Enrolled (Hundreds)')
plt.show()

The histogram above shows the University's total enrollment, which is fairly consistent. This is a uniform distribution and is what the University wants to see. Total enrollment is staying at a good level.

Now let's take a look at the enrollment specifically for students seeking a degree in History:

history_enrollment = [1, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5]

plt.hist(history_enrollment, bins=5, range=(1, 6))
plt.title('Student Enrollment (History Department)')
plt.xlabel('Years Enrolled')
plt.ylabel('Students Enrolled (Tens)')
plt.show()

What does this histogram tell us? Well this is somewhat skewed left dataset, we can see that there are a lot more students who have been enrolled for 3 or 4 years over 1 and 2 years. This indicates that the History program is becoming less popular. Where are all the students going then?

The school recently invested a lot of money in a new building for the Computer Science Department. Let's take a look at enrollment and see if the investment is paying off.

cs_enrollment = [1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 4, 4]

plt.hist(cs_enrollment, bins=5, range=(1, 6))
plt.title('Student Enrollment (Computer Science Department)')
plt.xlabel('Years Enrolled')
plt.ylabel('Students Enrolled (Tens)')
plt.show()

It looks like enrollment has skyrocketed for the Computer Science department in recent years. This could be because the University invested in the department, or it could be a sign that the sought after job skills in the real world are changing. Whatever the reason, the histograms let us clearly see the trends.

Interested applicants would like to know what kinds of SAT scores accepted students had. You've previously calculated that the mean score is 1250, with a standard deviation of 50.

Rather than gather every students score, you take what you know about the data and use a random number generator to generate a model.

sat_scores = np.random.normal(1250, 50, size=100000)

plt.hist(sat_scores, bins=1000, range=(800,1600))
plt.title('Admitted Student SAT Scores')
plt.xlabel('SAT Score')
plt.ylabel('Students')
plt.show()

95% of Students score within two standard deviations of the mean. An interested student scores an 1130 and wants to know if they are within that range.

mean = 1250
one_std = 50

two_below = (mean - 2*one_std)
print two_below

1150

Looks like they're just below it! Better re-take that test.

One of the big draws to your school is your excellent ultimate frisbee team. The team wins about 70% of their 50 games each season, or 35 games. An interested applicant wants to know what the chance is that they could improve their record to 40 games. You use what you know about binomial distributions to calculate the probability of such an occurence:

ultimate = np.random.binomial(50, 0.70, size=10000)
plt.hist(ultimate, range=(0, 50), bins=50, normed=True)
plt.xlabel('Number of Games')
plt.ylabel('Frequency')
plt.show()

Since it's a little hard to see from the graph, let's calculate exactly what chance they have of winning 40 games:

ultimate = np.random.binomial(50, 0.70, size=10000)
np.mean(ultimate == 40)

0.041000000000000002

Hmm, looks like it might be tough for the team to reach that number of wins, given the current data - but even more of a reason for this applicant to sign up and help the team improve!


\section{Histograms}

When we first look at a dataset, we want to be able to quickly understand certain things about it:

    Do some values occur more often than others?
    What is the range of the dataset (i.e., the min and the max values)?
    Are there a lot of outliers?

We can visualize this information using a chart called a histogram.

For instance, suppose that we have the following dataset:

d = [1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 5]

A simple histogram might show us how many 1's, 2's, 3's, etc. we have in this dataset.
Value 	Number of Samples
1 	3
2 	5
3 	2
4 	4
5 	1

When graphed, our histogram would look like this:


Suppose we had a larger dataset with values ranging from 0 to 50. We might not want to know exactly how many 0's, 1's, 2's, etc. we have.

Instead, we might want to know how many values fall between 0 and 5, 6 and 10, 11 and 15, etc.

These groupings are called bins. All bins in a histogram are always the same size. The width of each bin is the distance between the minimum and maximum values of each bin. In our example, the width of each bin would be 5.

For a dataset like this, our histogram table would look like this:
Bin 	Number of Values
(0, 5) 	2
(6, 10) 	10
(11, 15) 	11
... 	...
(46, 50) 	3

And if we were to graph this histogram, it would look like this: 


We can graph histograms using a Python module known as Matplotlib. We're not going to go into detail about Matplotlib’s plotting functions, but if you're interested in learning more, take our course Introduction to Matplotlib.

For now, familiarize yourself with the following syntax to draw a histogram:

# This imports the plotting package.  We only need to do this once.
from matplotlib import pyplot as plt 

# This plots a histogram
plt.hist(data)

# This displays the histogram
plt.show()

When we enter plt.hist with no keyword arguments, matplotlib will automatically make a histogram with 10 bins of equal width that span the entire range of our data.

If you want a different number of bins, use the keyword bins. For instance, the following code would give us 5 bins, instead of 10:

plt.hist(data, bins=5)

If you want a different range, you can pass in the minimum and maximum values that you want to histogram using the keyword range. We pass in a tuple of two numbers. The first number is the minimum value that we want to plot and the second value is the number that we want to plot up to, but not including.

For instance, if our dataset contained values between 0 and 100, but we only wanted to histogram numbers between 20 and 50, we could use this command:

# We pass 51 so that our range includes 50
plt.hist(data, range=(20, 51))

Here’s a complete example:

from matplotlib import pyplot as plt

d = np.array([1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 5])

plt.hist(d, bins=5, range=(1, 6))

plt.show()


\section{Examples of Distributions}
Histograms and their datasets can be classified based on the shape of the graphed values. In the next two exercises, we'll look at two different ways of describing histograms.

One way to classify a dataset is by counting the number of distinct peaks present in the graph. Peaks represent concentrations of data. Let's look at the following examples:

A unimodal dataset has only one distinct peak. histogram

A bimodal dataset has two distinct peaks. This often happens when the data contains two different populations. histogram

A multimodal dataset has more than two peaks. histogram

A uniform dataset doesn't have any distinct peaks. 

Most of the datasets that we'll be dealing with will be unimodal (one peak). We can further classify unimodal distributions by describing where most of the numbers are relative to the peak.

A symmetric dataset has equal amounts of data on both sides of the peak. Both sides should look about the same. histogram

A skew-right dataset has a long tail on the right of the peak, but most of the data is on the left. histogram

A skew-left dataset has a long tail on the left of the peak, but most of the data is on the right. histogram

The type of distribution affects the position of the mean and median. In heavily skewed distributions, the mean becomes a less useful measurement.

SYMMETRIC histogram

SKEW-RIGHT histogram

SKEW-LEFT histogram

\section{Normal Distributions}
The most common distribution in statistics is known as the normal distribution, which is a symmetric, unimodal distribution.

Lots of things follow a normal distribution:

    The heights of a large group of people
    Blood pressure measurements for a group of healthy people
    Errors in measurements

Normal distributions are defined by their mean and standard deviation. The mean sets the "middle" of the distribution, and the standard deviation sets the "width" of the distribution. A larger standard deviation leads to a wider distribution. A smaller standard deviation leads to a skinnier distribution.

Here are a few examples of normal distributions with different means and standard deviations:

normal_distribution

As we can see, each set of data has the same "shape", but with slight differences depending on their mean and standard deviation.

We can generate our own normally distributed datasets using NumPy. Using these datasets can help us better understand the properties and behavior of different distributions. We can also use them to model results, which we can then use as a comparison to real data.

In order to create these datasets, we need to use a random number generator. The NumPy library has several functions for generating random numbers, including one specifically built to generate a set of numbers that fit a normal distribution: np.random.normal. This function takes the following keyword arguments:

    loc: the mean for the normal distribution
    scale: the standard deviation of the distribution
    size: the number of random numbers to generate

a = np.random.normal(0, 1, size=100000)

If we were to plot this set of random numbers as a histogram, it would look like this: 

\section{Standard Deviation and Normal Distribution}
We know that the standard deviation affects the "shape" of our normal distribution. The last exercise helps to give us a more quantitative understanding of this.

Suppose that we have a normal distribution with a mean of 50 and a standard deviation of 10. When we say "within one standard deviation of the mean", this is what we are saying mathematically:

lower_bound = mean - std
            = 50 - 10
            = 40

upper_bound = mean + std
            = 50 + 10
            = 60

It turns out that we can expect about 68% of our dataset to be between 40 and 60, for this distribution.

As we saw in the previous exercise, no matter what mean and standard deviation we choose, 68% of our samples will fall between +/- 1 standard deviation of the mean!

In fact, here are a few more helpful rules for normal distributions:

    68% of our samples will fall between +/- 1 standard deviation of the mean
    95% of our samples will fall between +/- 2 standard deviations of the mean
    99.7% of our samples will fall between +/- 3 standard deviations of the mean

\section{Binomial Distribution}

It's known that a certain basketball player makes 30% of his free throws. On Friday night’s game, he had the chance to shoot 10 free throws. How many free throws might you expect him to make? We would expect 0.30 * 10 = 3.

However, he actually made 4 free throws out of 10 or 40%. Is this surprising? Does this mean that he’s actually better than we thought?

The binomial distribution can help us. It tells us how likely it is for a certain number of “successes” to happen, given a probability of success and a number of trials.

In this example:

    The probability of success was 30% (he makes 30% of free throws)
    The number of trials was 10 (he took 10 shots)
    The number of successes was 4 (he made 4 shots)

The binomial distribution is important because it allows us to know how likely a certain outcome is, even when it's not the expected one. From this graph, we can see that it's not that unlikely an outcome for our basketball player to get 4 free throws out of 10. However, it would be pretty unlikely for him to get all 10.

binomiall

There are some complicated formulas for determining these types of probabilities. Luckily for us, we can use NumPy - specifically, its ability to generate random numbers. We can use these random numbers to model a distribution of numerical data that matches the real-life scenario we're interested in understanding.

For instance, suppose we want to know the different probabilities of our basketball player making 1, 2, 3, etc. out of 10 shots.

NumPy has a function for generating binomial distributions: np.random.binomial, which we can use to determine the probability of different outcomes.

The function will return the number of successes for each "experiment".

It takes the following arguments:

    N: The number of samples or trials
    P: The probability of success
    size: The number of experiments

Let's generate a bunch of "experiments" of our basketball player making 10 shots. We choose a big N to be sure that our probabilities converge on the correct answer.

# Let's generate 10,000 "experiments"
# N = 10 shots
# P = 0.30 (30% he'll get a free throw)

a = np.random.binomial(10, 0.30, size=10000)

Now we have a record of 10,000 experiments. We can use Matplotlib to plot the results of all of these experiments:

plt.hist(a, range=(0, 10), bins=10, normed=True)
plt.xlabel('Number of "Free Throws"')
plt.ylabel('Frequency')
plt.show()

binomiall


\section{Binomial Distribution and Probability}
Let's return to our original question:

Our basketball player has a 30% chance of making any individual basket. He took 10 shots and made 4 of them, even though we only expected him to make 3. What percent chance did he have of making those 4 shots?

We can calculate a different probability by counting the percent of experiments with the same outcome, using the np.mean function.

Remember that taking the mean of a logical statement will give us the percent of values that satisfy our logical statement.

Let's calculate the probability that he makes 4 baskets:

a = np.random.binomial(10, 0.30, size=10000)
np.mean(a == 4)

When we run this code, we might get:

>> 0.1973

Remember, because we're using a random number generator, we'll get a slightly different result each time. With the large *size we chose, the calculated probability should be accurate to about 2 decimal places.*

So, our basketball player has a roughly 20% chance of making 4 baskets.

This suggests that what we observed wasn't that unlikely. It's quite possible that he hasn't got any better; he just got lucky.

\section{Review}
Let's review! In this lesson, you learned how to use NumPy to analyze different distributions and generate random numbers to produce datasets. Here's what we covered:

    What is a histogram and how to map one using Matplotlib
    How to identify different dataset shapes, depending on peaks or distribution of data
    The definition of a normal distribution and how to use NumPy to generate one using NumPy's random number functions
    The relationships between normal distributions and standard deviations
    The definition of a binomial distribution

Now you can use NumPy to analyze and graph your own datasets! You should practice building your intuition about not only what the data says, but what conclusions can be drawn from your observations.

np.random.normal(loc = 16.3, scale = 3.3, size = 1000)


\section{Quiz}
Which of the following are the correct keyword arguments for generating a random distribution using np.random.binomial?
N, P, size

What is a histogram?
A chart that creates equally spaced bins and counts how many values from our dataset fall into each bin.

How many peaks does a unimodal dataset have?
One

In a normal distribution, how much of the data lies within one standard deviation?
68%


The average height of a male giraffe is 16.3 feet with a standard deviation of 3.3 feet. Which of the following will generate a random distribution of 1000 male giraffe heights using np.random.normal?
np.random.normal(loc = 16.3, scale = 3.3, size = 1000)


Why do we use binomial distributions?
Because they are effective at helping us understand the different probabilities that an event will occur.


\section{Project Election}
import codecademylib
import numpy as np
from matplotlib import pyplot as plt

survey_responses = ['Ceballos', 'Kerrigan', 'Ceballos', 'Ceballos', 'Ceballos','Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Ceballos', 
'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Ceballos', 'Ceballos', 'Ceballos', 'Ceballos',
'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Ceballos',
'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Ceballos', 'Ceballos', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Kerrigan', 'Ceballos']



total_ceballos = sum([1 for n in survey_responses if n == 'Ceballos'])
print(total_ceballos)

percentage_ceballos = 100 * total_ceballos / float(len(survey_responses))

print(percentage_ceballos)

possible_surveys = np.random.binomial(len(survey_responses), 0.54, size=10000) / float(len(survey_responses))

plt.hist(possible_surveys, range=(0, 1), bins=20)
plt.show()

ceballos_loss_surveys = np.mean(possible_surveys < 0.5)*100

print(ceballos_loss_surveys)

possible_surveys_len = float(len(possible_surveys))
incorrect_predictions = len(possible_surveys[possible_surveys < 0.5])
ceballos_loss_surveys_2 = incorrect_predictions / possible_surveys_len
print(ceballos_loss_surveys_2)

large_survey = np.random.binomial(float(7000), 0.54, size=10000) / float(7000)

ceballos_loss_new = np.mean(large_survey < 0.5)
print(ceballos_loss_new)


large_surveys_len = float(len(large_survey))
incorrect_predictions = len(large_survey[large_survey < 0.5])
ceballos_loss_surveys_new_2 = incorrect_predictions / large_surveys_len
print(ceballos_loss_surveys_new_2)

\section{Project Muncies}
import codecademylib
import numpy as np

calorie_stats = np.genfromtxt('cereal.csv', delimiter=',')
calorie_stats_sorted = np.sort(calorie_stats)

average_calories = np.mean(calorie_stats)
median_calories = np.median(calorie_stats_sorted)
nth_percentile = np.percentile(calorie_stats_sorted, 4)
more_calories = np.mean(calorie_stats > 60)*100
calorie_std = np.std(calorie_stats)

'''
CrunchieMunchies are lit. They are more healthy than 96% of the competitors, where the average falls around about 110 calories, compared to the mind blowing 60 of CrunchieMunchies. Other cereals deviate slightly from the mean, but even so, the CrunchieMunchies crush all the statistics.
'''

print(calorie_std)
print(more_calories)
print(nth_percentile)
print(average_calories)
print(median_calories)
print(calorie_stats_sorted)

\subsection{csv}
70, 120, 70, 50, 110 , 110, 110, 130, 90, 90, 120, 110, 120, 110, 110, 110, 100, 110, 110, 110, 100, 110, 100, 100, 110, 110, 100, 120, 120, 110, 100, 110, 100, 110, 120, 120, 110, 110, 110, 140, 110, 100, 110, 100, 150, 150, 160, 100, 120, 140, 90, 130, 120, 100, 50, 50, 100, 100, 120, 100, 90, 110, 110, 80, 90, 90, 110, 110, 90, 110, 140, 100, 110, 110, 100, 100, 110

\section{Bakery}
import numpy as np

#(Flour|Sugar|Eggs|Milk|Butter)
cupcakes = np.array([2, 0.75, 2, 1, 0.5])

recipes = np.genfromtxt('recipes.csv', delimiter=',')

print(recipes)

eggs = recipes[:,2]

print(eggs == 1)

cookies = recipes[2,:]

print(cookies)

double_batch = cupcakes*2

grocery_list = double_batch+cookies

print(grocery_list)

\subsection{csv}
2,0.75,2,1,0.5
1,0.125,1,1,0.125
2.75,1.5,1,0,1
4,0.5,2,2,0.5

\section{Introduction To ScyPy}
Say you work for a major social media website. Your boss always says "data drives all our decisions" and it seems to be true. Metrics are collected on all users of your website, terabytes of data stored in replicated databases.

One day, your boss wants to know if college students are engaging in your website. You pull up the records for users in that age bracket and look at them one by one. The first person only spent half a second on your website before closing the tab — that doesn't look good. But the second person was on the site for thirty minutes! That's a running average of 15 minutes site time per user, but you still have half a million records to look at.

On top of that, you need to compare it against other age brackets (and the average overall). That's going to take a lot of time if you do it all by hand, and you're still not sure what your methodology for proving college students spend enough time on your website to be "engaged".

When conducting data analysis, we want to say something meaningful about our data. Often, we want to know if a change or difference we see in a dataset is "real" or if it’s just a normal fluctuation or a result of the specific sample of people we have chosen to measure. A difference we observe in our data is only important if we can be reasonably sure that it is representative of the population as a whole, and reasonably sure that our result is repeatable.

This question of whether a difference is significant or not is essential to making decisions based on that difference. Some instances where this might come up include:

    Performing an A/B test — are the different observations really the results of different conditions (i.e., Condition A vs. Condition B)? Or just the result of random chance?
    Conducting a survey — is the fact that men gave slightly different responses than women a real difference between men and women? Or just the result of chance?

In this lesson, we will cover the fundamental concepts that will help us create tests to measure our confidence in our statistical results:

    Sample means and population means
    The Central Limit Theorem
    Why we use hypothesis tests
    What errors we can come across and how to classify them




Are the Millennials Engaged?

You work at the global megacorp social network SpyPy. SpyPy has 1.5 billion daily users, and you want to make sure that people in the millennial age bracket are engaging with your website. Your boss seems particularly frazzled by this question, and he's put it on you to find out. You decide that "engagement" means spending more than the average of seven minutes on the website. You fire up your data-science stack in Python and first check the average time -- which turns out to be near 11 whole minutes! But you can't really tell if they're really spending more time or if it's just random chance that a few of your users left the browser open and walked away. You write the following code:

import spypy
from scipy.stats import ttest_1samp

millennial_times = spypy.get_site_times_for_demographic('millennial')
t_stat, p_val = ttest_1samp(millennial_times, 7)

if p_val < .05:
    print "The Millennials are engaged!"
else:
    print "The Millennials are not engaged :(!"

The Millennials are engaged!

SpyPy: We're Significantly Different

Well that's great news! Millennials are, for the most part, spending around 10 minutes on your website. But before you break out the champagne glasses your boss is in a frenzy again, this time about Metropolitan Statistical Areas (MSAs). You are tasked with finding if people in cooler climates post more pictures on SpyPy than people in warmer climates. You cross corroborate with weather data and run a statistical test on the info.

from scipy.stats import ttest_ind

warmer_weather_picture_count = spypy.get_number_pictures_for_climate('hot')
colder_weather_picture_count = spypy.get_number_pictures_for_climate('cold')

t_stat, p_val = ttest_ind(warmer_weather_picture_count, colder_weather_picture_count)

if p_val < .05:
    print "People from colder climates post a different number of pictures compared to people from warmer climates"
else:
    print "Climate doesn't appear to affect the number of pictures posted"

Climate doesn't appear to affect the number of pictures posted

SpyPy: Because We Care About Your Data

Seems like climate probably doesn't really affect the number of times people post pictures. Not really sure why that would've been the case anyway. SpyPy has a new feature that you think will get people to interact with the website for longer: SpyPy Stories. It is preliminarily being launched to 8 million users and the internal goal is to get 2 million people to post SpyPy Stories in the first week. Unfortunately, only 1,997,893 people posted SpyPy Stories this week. We want to know if this is a significant difference from our goal -- did we pretty much meet it or did we seriously miss? You know how to answer this question:

from scipy.stats import binom_test

number_of_trials = 8000000
expected_successes = 2000000
actual_successes = 1997893
expected_success_rate = float(expected_successes) / float(number_of_trials)

p_val = binom_test(actual_successes, n=number_of_trials, p=expected_success_rate)
if p_val < 0.05:
    print "We didn't hit our target by a significant amount"
else:
    print "We just missed our target by a very small amount!"

We just missed our target by a very small amount!

Looks like we came very close to hitting our target for SpyPy Stories! You've saved the day so many times already! Your boss comes by to thank you for all the hard work you put in today and says you've made significant contributions to the team. You tell her you're not sure if that's true, but you definitely have a way of finding out.


\section{Sample and Population Means}

Suppose you want to know the average height of an oak tree in your local park. On Monday, you measure 10 trees and get an average height of 32 ft. On Tuesday, you measure 12 different trees and reach an average height of 35 ft. On Wednesday, you measure the remaining 11 trees in the park, whose average height is 31 ft. Overall, the average height for all trees in your local park is 32.8 ft.


The individual measurements on Monday, Tuesday, and Wednesday are called samples. A sample is a subset of the entire population. The mean of each sample is the sample mean and it is an estimate of the population mean.



Note that the sample means (32 ft., 35 ft., and 31 ft.) were all close to the population mean (32.8 ft.), but were all slightly different from the population mean and from each other.


For a population, the mean is a constant value no matter how many times it's recalculated. But with a set of samples, the mean will depend on exactly what samples we happened to choose. From a sample mean, we can then extrapolate the mean of the population as a whole. There are many reasons we might use sampling, such as:

\begin{itemize}
	\item We don't have data for the whole population.
	\item We have the whole population data, but it is so large that it is infeasible to analyze.
	\item We can provide meaningful answers to questions faster with sampling.
\end{itemize}

When we have a numerical dataset and want to know the average value, we calculate the mean. For a population, the mean is a constant value no matter how many times it's recalculated. But with a set of samples, the mean will depend on exactly what samples we happened to choose. From a sample mean, we can then extrapolate the mean of the population as a whole.\\

\section{Central Limit Theorem}
Perhaps, this time, you're a tailor of school uniforms at a middle school. You need to know the average height of people from 10-13 years old in order to know which sizes to make the uniforms. Knowing the best decisions are based on data, you set out to do some research at your local middle school.

Organizing with the school, you measure the heights of some students. Their average height is 57.5 inches. You know a little about sampling and decide that measuring 30 out of the 300 students gives enough data to assume 57.5 inches is roughly the average height of everyone at the middle school. You set to work with this dimension and make uniforms that fit people of this height, some smaller and some larger.

Unfortunately, when you go about making your uniforms many reports come back saying that they are too small. Something must have gone wrong with your decision-making process! You go back to collect the rest of the data: you measure the sixth graders one day (56.7, not so bad), the seventh graders after that (59 inches tall on average), and the eighth graders the next day (61.7 inches!). Your sample mean was so far off from your population mean. How did this happen?

Well, your sample selection was skewed to one direction of the total population. It looks like you must have measured more sixth graders than is representative of the whole middle school. How do you get an average sample height that looks more like the average population height?

In the previous exercise, we looked at different sets of samples taken from a population and how the mean of each set could be different from the population mean. This is a natural consequence of the fact that a set of samples has less data than the population to which it belongs. If our sample selection is poor then we will have a sample mean seriously skewed from our population mean.

There is one surefire way to mitigate the risk of having a skewed sample mean — take a larger set of samples. The sample mean of a larger sample set will more closely approximate the population mean. This phenomenon, known as the Central Limit Theorem, states that if we have a large enough sample size, all of our sample means will be sufficiently close to the population mean.

Later, we'll learn how to put numeric values on "large enough" and "sufficiently close".

\section{Hypothesis Tests}
When observing differences in data, a data analyst understands the possibility that these differences could be the result of random chance.

Suppose we want to know if men are more likely to sign up for a given programming class than women. We invite 100 men and 100 women to this class. After one week, 34 women sign up, and 39 men sign up. More men than women signed up, but is this a "real" difference?

We have taken sample means from two different populations, men and women. We want to know if the difference that we observe in these sample means reflects a difference in the population means. To formally answer this question, we need to re-frame it in terms of probability:

"What is the probability that men and women have the same level of interest in this class and that the difference we observed is just chance?"

In other words, "If we gave the same invitation to every person in the world, would more men still sign up?"

A more formal version is: "What is the probability that the two population means are the same and that the difference we observed in the sample means is just chance?"

These statements are all ways of expressing a null hypothesis. A null hypothesis is a statement that the observed difference is the result of chance.

Hypothesis testing is a mathematical way of determining whether we can be confident that the null hypothesis is false. Different situations will require different types of hypothesis testing, which we will learn about in the next lesson.

\section{Type I or Type II}
When we rely on automated processes to make our decisions for us, we need to be aware of how this automation can lead to mistakes. Computer programs are as fallible as the humans who design them. As humans capable of programming, the responsibility is on us to understand what can go wrong and what we can do to contain these foreseeable problems.

In statistical hypothesis testing, we concern ourselves primarily with two types of error. The first kind of error, known as a Type I error, is finding a correlation between things that are not related. This error is sometimes called a "false positive" and occurs when the null hypothesis is rejected even though it is true.

For example, let's say you conduct an A/B test for an online store and conclude that interface B is significantly better than interface A at directing traffic to a checkout page. You have rejected the null hypothesis that there is no difference between the two interfaces. If, in reality, your results were due to the groups you happened to pick, and there is actually no significant difference between interface A and interface B in the greater population, you have been the victim of a false positive.

The second kind of error, a Type II error, is failing to find a correlation between things that are actually related. This error is referred to as a "false negative" and occurs when the null hypothesis is accepted even though it is false.

For example, with the A/B test situation, let's say that after the test, you concluded that there was no significant difference between interface A and interface B. If there actually is a difference in the population as a whole, your test has resulted in a false negative.

\section{P-Values}
We have discussed how a hypothesis test is used to determine the validity of a null hypothesis. A hypothesis test provides a numerical answer, called a p-value, that helps us decide how confident we can be in the result. In this context, a p-value is the probability that we yield the observed statistics under the assumption that the null hypothesis is true.

A p-value of 0.05 would mean that there is a 5% chance that the null hypothesis is true. This generally means there is a 5% chance that there is no difference between the two population means.

Before conducting a hypothesis test, we determine the necessary threshold we would need before concluding that the results are significant. A higher p-value is more likely to give a false positive so if we want to be very sure that the result is not due to just chance, we will select a very small p-value.

It is important that we choose the significance level before we perform our statistical hypothesis tests to yield a p-value. If we wait until after we see the results, we might pick our threshold such that we get the result we want to see. For instance, if we're trying to publish our results, we might set a significance level that makes our results seem statistically significant. Choosing our significance level in advance helps keep us honest.

Generally, we want a p-value of less than 0.05, meaning that there is less than a 5% chance that our results are due to random chance.

\section{Examples }
Suppose we were exploring the relationship between local honey and allergies. Which of these would be a statement of the null hypothesis? Local honey has no effect on allergies, any relationship between consuming local honey and allergic outbreaks is due to chance. Correct! The null hypothesis states that any difference observed within sample means is coincidental.

Which of the following describes a Type II error?
A survey on preferred ice cream flavors not establishing a clear favorite when the majority of people prefer chocolate.

Which of these describes a sample mean?
The mean of a subset of our population which is hopefully, but not necessarily, representative of the overall average.

What is a p-value?
In a hypothesis test, a p-value is the probability that the null hypothesis is true.

Which of the following hypothesis tests would be used to compare two sets of numerical data?

Chi Square x
1 Sample T-Test x
ANOVA x
2 Sample T-Test  

Analysis of variance is used to determine if three or more numerical samples come from the same population.


Which of these is an accurate statement of the Central Limit Theorem?
For a large enough sample size, our sample mean will be sufficiently close to the population mean.


What is a statistical hypothesis test?
A way of quantifying the truth of a statement.

ANOVA is a type of hypothesis test, but does not cover all types of hypothesis test.


\section{Key Points}
Let's take a second and review. In this lesson, you learned the basics of the NumPy package. Here are some key points:

    Arrays are a special type of list that allows us to store values in an organized manner.
    An array can be created by either defining it directly using np.array() or by importing a CSV using np.genfromtxt('file.csv', delimiter=',').
    An operation (such as addition) can be performed on every element in an array by simply performing it on the array itself.
    Elements can be selected from arrays using their index and array locations, both of which start at 0.
    Logical operations can be used to create new, more focused arrays out of larger arrays.

The next lesson will explore how to analyze these arrays and use means, medians, and standard deviations to tell a story. But first, practice what you've learned by working through the following checkpoints.


\section{Hypothesis Tests}
When we are trying to compare datasets, we often need a way to be confident knowing if datasets are significantly different from each other.
Some situations involve correlating numerical data, such as:

    a professor expects an exam average to be roughly 75%, and wants to know if the actual scores line up with this expectation. Was the test actually too easy or too hard?
    a manager of a chain of stores wants to know if certain locations have different revenues on different days of the week. Are the revenue differences a result of natural fluctuations or a significant difference between the stores' sales patterns?
    a PM for a website wants to compare the time spent on different versions of a homepage. Does one version make users stay on the page significantly longer?

Others involve categorical data, such as:

    a pollster wants to know if men and women have significantly different yogurt flavor preferences. Does a result where men more often answer "chocolate" as their favorite reflect a significant difference in the population?
    do different age groups have significantly different emotional reactions to different ads?

In this lesson, you will learn how about how we can use hypothesis testing to answer these questions. There are several different types of hypothesis tests for the various scenarios you may encounter. Luckily, SciPy has built-in functions that perform all of these tests for us, normally using just one line of code.

For numerical data, we will cover:
\begin{itemize}
    \item One Sample T-Tests
    \item Two Sample T-Tests
    \item ANOVA
    \item Tukey Tests
\end{itemize}


For categorical data, we will cover:
\begin{itemize}
    \item Binomial Tests
    \item Chi Square
\end{itemize}


After this lesson, you will have a wide range of tools in your arsenal to find meaningful correlations in data.

\subsection{Sample T-Testing}
Let's imagine the fictional business BuyPie, which sends ingredients for pies to your household, so that you can make them from scratch. Suppose that a product manager wants the average age of visitors to BuyPie.com to be 30. In the past hour, the website had 100 visitors and the average age was 31. Are the visitors too old? Or is this just the result of chance and a small sample size?

We can test this using a univariate T-test. A univariate T-test compares a sample mean to a hypothetical population mean. It answers the question "What is the probability that the sample came from a distribution with the desired mean?"

When we conduct a hypothesis test, we want to first create a null hypothesis, which is a prediction that there is no significant difference. The null hypothesis that this test examines can be phrased as such: "The set of samples belongs to a population with the target mean".

The result of the 1 Sample T Test is a p-value, which will tell us whether or not we can reject this null hypothesis. Generally, if we receive a p-value of less than 0.05, we can reject the null hypothesis and state that there is a significant difference.

SciPy has a function called ttest_1samp, which performs a 1 Sample T-Test for you.

ttest_1samp requires two inputs, a distribution of values and an expected mean:

tstat, pval = ttest_1samp(example_distribution, expected_mean)
print pval

It also returns two outputs: the t-statistic (which we won't cover in this course), and the p-value — telling us how confident we can be that the sample of values came from a distribution with the mean specified.

In the last exercise, we got a p-value that was much higher than 0.05, so we cannot reject the null hypothesis. Does this mean that if we wait for more visitors to BuyPie, the average age would definitely be 30 and not 31? Not necessarily. In fact, in this case, we know that the mean of our sample was 31.

P-values give us an idea of how confident we can be in a result. Just because we don’t have enough data to detect a difference doesn’t mean that there isn’t one. Generally, the more samples we have, the smaller a difference we’ll be able to detect. You can learn more about the exact relationship between the number of samples and detectable differences in the Sample Size Determination course.

To gain some intuition on how our confidence levels can change, let's explore some distributions with different means and how our p-values from the 1 Sample T-Tests change.

\subsection{2 Sample T-Test}
Suppose that last week, the average amount of time spent per visitor to a website was 25 minutes. This week, the average amount of time spent per visitor to a website was 28 minutes. Did the average time spent per visitor change? Or is this part of natural fluctuations?

One way of testing whether this difference is significant is by using a 2 Sample T-Test. A 2 Sample T-Test compares two sets of data, which are both approximately normally distributed.

The null hypothesis, in this case, is that the two distributions have the same mean.

We can use SciPy's ttest_ind function to perform a 2 Sample T-Test. It takes the two distributions as inputs and returns the t-statistic (which we don't use), and a p-value. If you can't remember what a p-value is, refer to the earlier exercise on univariate t-tests.

\subsection{Dangers of Multiple T-Tests}
Suppose that we own a chain of stores that sell ants, called VeryAnts. There are three different locations: A, B, and C. We want to know if the average ant sales over the past year are significantly different between the three locations.

At first, it seems that we could perform T-tests between each pair of stores.

We know that the p-value is the probability that we incorrectly reject the null hypothesis on each t-test. The more t-tests we perform, the more likely that we are to get a false positive, a Type I error.

For a p-value of 0.05, if the null hypothesis is true then the probability of obtaining a significant result is 1 – 0.05 = 0.95. When we run another t-test, the probability of still getting a correct result is 0.95 * 0.95, or 0.9025. That means our probability of making an error is now close to 10%! This error probability only gets bigger with the more t-tests we do.

from scipy.stats import ttest_ind
import numpy as np

a = np.genfromtxt("store_a.csv",  delimiter=",")
b = np.genfromtxt("store_b.csv",  delimiter=",")
c = np.genfromtxt("store_c.csv",  delimiter=",")

a_mean = np.mean(a)
b_mean = np.mean(b)
c_mean = np.mean(c)

a_std = np.std(a)
b_std = np.std(b)
c_std = np.std(c)

a_b_tval, a_b_pval = ttest_ind(a,b)
a_c_tval, a_c_pval = ttest_ind(a,c)
c_b_tval, b_c_pval = ttest_ind(b,c)

print(a_b_pval)
print(a_c_pval)
print(b_c_pval)

error_prob = (1-0.95**3)

print(error_prob)

\subsection{ANOVA}
In the last exercise, we saw that the probability of making a Type I error got dangerously high as we performed more t-tests.

When comparing more than two numerical datasets, the best way to preserve a Type I error probability of 0.05 is to use ANOVA. ANOVA (Analysis of Variance) tests the null hypothesis that all of the datasets have the same mean. If we reject the null hypothesis with ANOVA, we're saying that at least one of the sets has a different mean; however, it does not tell us which datasets are different.

We can use the SciPy function f_oneway to perform ANOVA on multiple datasets. It takes in each dataset as a different input and returns the t-statistic and the p-value. For example, if we were comparing scores on a videogame between math majors, writing majors, and psychology majors, we could run an ANOVA test with this line:

fstat, pval = f_oneway(scores_mathematicians, scores_writers, scores_psychologists)

The null hypothesis, in this case, is that all three populations have the same mean score on this videogame. If we reject this null hypothesis (if we get a p-value less than 0.05), we can say that we are reasonably confident that a pair of datasets is significantly different. After using only ANOVA, we can't make any conclusions on which two populations have a significant difference.

Let's look at an example of ANOVA in action.

\subsection{Assumptions of Numerical Hypothesis Tests}
Before we use numerical hypothesis tests, we need to be sure that the following things are true:
1. The samples should each be normally distributed...ish

Data analysts in the real world often still perform hypothesis on sets that aren't exactly normally distributed. What is more important is to recognize if there is some reason to believe that a normal distribution is especially unlikely. If your dataset is definitively not normal, the numerical hypothesis tests won't work as intended.

For example, imagine we have three datasets, each representing a day of traffic data in three different cities. Each dataset is independent, as traffic in one city should not impact traffic in another city. However, it is unlikely that each dataset is normally distributed. In fact, each dataset probably has two distinct peaks, one at the morning rush hour and one during the evening rush hour. The histogram of a day of traffic data might look something like this:

histogram

In this scenario, using a numerical hypothesis test would be inappropriate.
2. The population standard deviations of the groups should be equal

For ANOVA and 2-Sample T-Tests, using datasets with standard deviations that are significantly different from each other will often obscure the differences in group means.

To check for similarity between the standard deviations, it is normally sufficient to divide the two standard deviations and see if the ratio is "close enough" to 1. "Close enough" may differ in different contexts but generally staying within 10% should suffice.
3. The samples must be independent

When comparing two or more datasets, the values in one distribution should not affect the values in another distribution. In other words, knowing more about one distribution should not give you any information about any other distribution.

Here are some examples where it would seem the samples are not independent:

    the number of goals scored per soccer player before, during, and after undergoing a rigorous training regimen
    a group of patients' blood pressure levels before, during, and after the administration of a drug

It is important to understand your datasets before you begin conducting hypothesis tests on it so that you know you are choosing the right test.

\subsection{Tukeys Range Test}
Let's say that we have performed ANOVA to compare three sets of data from the three VeryAnts stores. We received the result that there is some significant difference between datasets.

Now, we have to find out which datasets are different.

We can perform a Tukey's Range Test to determine the difference between datasets.

If we feed in three datasets, such as the sales at the VeryAnts store locations A, B, and C, Tukey's Test can tell us which pairs of locations are distinguishable from each other.

The function to perform Tukey's Range Test is pairwise_tukeyhsd, which is found in statsmodel, not scipy. We have to provide the function with one list of all of the data and a list of labels that tell the function which elements of the list are from which set. We also provide the significance level we want, which is usually 0.05.

For example, if we were looking to compare mean scores of movies that are dramas, comedies, or documentaries, we would make a call to pairwise_tukeyhsd like this:

movie_scores = np.concatenate([drama_scores, comedy_scores, documentary_scores])
labels = ['drama'] * len(drama_scores) + ['comedy'] * len(comedy_scores) + ['documentary'] * len(documentary_scores)

tukey_results = pairwise_tukeyhsd(movie_scores, labels, 0.05)

It will return a table of information, telling you whether or not to reject the null hypothesis for each pair of datasets.

\subsection{Binomial Tests}
Let's imagine that we are analyzing the percentage of customers who make a purchase after visiting a website. We have a set of 1000 customers from this month, 58 of whom made a purchase. Over the past year, the number of visitors per every 1000 who make a purchase hovers consistently at around 72. Thus, our marketing department has set our target number of purchases per 1000 visits to be 72. We would like to know if this month's number, 58, is a significant difference from that target or a result of natural fluctuations.

How do we begin comparing this, if there's no mean or standard deviation that we can use? The data is divided into two discrete categories, "made a purchase" and "did not make a purchase".

So far, we have been working with numerical datasets. The tests we have looked at, the 1- and 2-Sample T-Tests, ANOVA, and Tukey's Range test, will not work if we can't find the means of our distributions and compare them.

If we have a dataset where the entries are not numbers, but categories instead, we have to use different methods.

To analyze a dataset like this, with two different possibilities for entries, we can use a Binomial Test. A Binomial Test compares a categorical dataset to some expectation.

Examples include:

    Comparing the actual percent of emails that were opened to the quarterly goals
    Comparing the actual percentage of respondents who gave a certain survey response to the expected survey response
    Comparing the actual number of heads from 1000 coin flips of a weighted coin to the expected number of heads

The null hypothesis, in this case, would be that there is no difference between the observed behavior and the expected behavior. If we get a p-value of less than 0.05, we can reject that hypothesis and determine that there is a difference between the observation and expectation.

SciPy has a function called binom_test, which performs a Binomial Test for you.

binom_test requires three inputs, the number of observed successes, the number of total trials, and an expected probability of success. For example, with 1000 coin flips of a fair coin, we would expect a "success rate" (the rate of getting heads), to be 0.5, and the number of trials to be 1000. Let's imagine we get 525 heads. Is the coin weighted? This function call would look like:

pval = binom_test(525, n=1000, p=0.5)

It returns a p-value, telling us how confident we can be that the sample of values was likely to occur with the specified probability. If we get a p-value less than 0.05, we can reject the null hypothesis and say that it is likely the coin is actually weighted, and that the probability of getting heads is statistically different than 0.5.

\subsection{Test}
You regularly order delivery from two different Pho restaurants, "What the Pho" and "Pho Tonic". You want to know if there's a significant difference between these two restaurants' average time to deliver to your house. What test could you use to determine this?
2 Sample T-Test


Let's say we run a 1 Sample T-Test on means for an exam. We expect the mean to be 75%, but we want to see if the actual scores are significantly better or worse than what we expected. After running the T-Test, we get a p-value of 0.25. What does this result mean?
We cannot confidently reject the null-hypothesis, so we do not have enough data to say that the mean on this exam is different from 75%.


Let's say that last month 7% of free users of a site converted to paid users, but this month only 5% of free users converted. What kind of test should we use to see if this difference is significant?
Chi square


Let's say we are comparing the time that users spend on three different versions of a landing page for a website. What test do we use to determine if there is a significant difference between any two of the sets?
ANOVA

You own a juice bar and you theorize that 75% of your customers live in the surrounding 5 blocks. You survey a random sample of 12 customers and find that 7 of them live within those 5 blocks. What test do you run to determine if your results significantly differ from your expectation?
Binomial Test

You've collected data on 1000 different sites that end with .com, .edu, and .org and have recorded the number of each that have Times New Roman, Helvetica, or another font as their main font. What test can you use to determine if there's a relationship between top-level domain and font type?
Chi Square


You've surveyed 10 people who work in finance, 10 people who work in education, and 10 people who work in the service industry on how many cups of coffee they drink per day. What test can you use to determine if there is a significant difference between the average coffee consumption of these three groups?
ANOVA

You just bought a new tea kettle that is supposed to heat water to boiling in 2 minutes. What kind of test can you run to determine if the time-to-boil is averaging significantly more than 2 minutes?
1 Sample T Test

If we perform an ANOVA test on 3 datasets and reject the null hypothesis, what test should we perform to determine which pairs of datasets are different?
Tukey's Range Test

What kind of test would you use to see if men and women identify differently as "Republican", "Democrat", or "Independent"?
Chi Square

\section{Pandas in Python}
Pandas is a Python module for working with tabular data (i.e., data in a table with rows and columns). Tabular data has a lot of the same functionality as SQL or Excel, but Pandas adds the power of Python.

In order to get access to the Pandas module, we'll need to install the module and then import it into a Python file. 

The pandas module is usually imported at the top of a Python file under the alias pd.

import pandas as pd

A DataFrame is an object that stores data as rows and columns. You can think of a DataFrame as a spreadsheet or as a SQL table. You can manually create a DataFrame or fill it with data from a CSV, an Excel spreadsheet, or a SQL query.

DataFrames have rows and columns. Each column has a name, which is a string. Each row has an index, which is an integer. DataFrames can contain many different data types: strings, ints, floats, tuples, etc.

You can pass in a dictionary to pd.DataFrame(). Each key is a column name and each value is a list of column values. The columns must all be the same length or you will get an error. Here’s an example:

df1 = pd.DataFrame({
    'name': ['John Smith', 'Jane Doe', 'Joe Schmo'],
    'address': ['123 Main St.', '456 Maple Ave.', '789 Broadway'],
    'age': [34, 28, 51]
})

This command creates a DataFrame called df1 that looks like this:


You can also add data using lists.

For example, you can pass in a list of lists, where each one represents a row of data. Use the keyword argument columns to pass a list of column names.

df2 = pd.DataFrame([
    ['John Smith', '123 Main St.', 34],
    ['Jane Doe', '456 Maple Ave.', 28],
    ['Joe Schmo', '789 Broadway', 51]
    ],
    columns=['name', 'address', 'age'])

This command produces a DataFrame df2 that looks like this:

Comma Separated Variables (CSV)

We now know how to create our own DataFrame. However, most of the time, we'll be working with datasets that already exist. One of the most common formats for big datasets is the CSV.

CSV (comma separated values) is a text-only spreadsheet format. You can find CSVs in lots of places:

    Online datasets (here's an example from data.gov)
    Export from Excel or Google Sheets
    Export from SQL

The first row of a CSV contains column headings. All subsequent rows contain values. Each column heading and each variable is separated by a comma:

column1,column2,column3
value1,value2,value3

That example CSV represents the following table:

Loading and Saving CSVs

When you have data in a CSV, you can load it into a DataFrame in Pandas using .read_csv():

pd.read_csv('my-csv-file.csv')

In the example above, the .read_csv() method is called. The CSV file called my-csv-file is passed in as an argument.

We can also save data to a CSV, using .to_csv().

df.to_csv('new-csv-file.csv')

In the example above, the .to_csv() method is called on df (which represents a DataFrame object). The name of the CSV file is passed in as an argument (new-csv-file.csv). By default, this method will save the CSV file in your current directory.


Inspect a DataFrame

When we load a new DataFrame from a CSV, we want to know what it looks like.

If it's a small DataFrame, you can display it by typing print(df).

If it's a larger DataFrame, it's helpful to be able to inspect a few items without having to look at the entire DataFrame.

The method .head() gives the first 5 rows of a DataFrame. If you want to see more rows, you can pass in the positional argument n. For example, df.head(10) would show the first 10 rows.

The method df.info() gives some statistics for each column.


Now we know how to create and load data. Let's select parts of those datasets that are interesting or important to our analyses.

Suppose you have the DataFrame called customers, which contains the ages of your customers:
name 	age
Rebecca Erikson 	35
Thomas Roberson 	28
Diane Ochoa 	42
... 	...

Perhaps you want to take the average or plot a histogram of the ages. In order to do either of these tasks, you'd need to select the column.

There are two possible syntaxes for selecting all values from a column:

    Select the column as if you were selecting a value from a dictionary using a key. In our example, we would type customers['age'] to select the ages.
    If the name of a column follows all of the rules for a variable name (doesn't start with a number, doesn't contain spaces or special characters, etc.), then you can select it using the following notation: df.MySecondColumn. In our example, we would type customers.age.

When we select a single column, the result is called a Series.

E.g.

import pandas as pd

df = pd.DataFrame([
  ['January', 100, 100, 23, 100],
  ['February', 51, 45, 145, 45],
  ['March', 81, 96, 65, 96],
  ['April', 80, 80, 54, 180],
  ['May', 51, 54, 54, 154],
  ['June', 112, 109, 79, 129]],
  columns=['month', 'clinic_east',
           'clinic_north', 'clinic_south',
           'clinic_west']
)

clinic_north = df.clinic_north
#clinic_north = df['clinic_north']

print(type(clinic_north))
print(type(df))


Selecting Multiple Columns

When you have a larger DataFrame, you might want to select just a few columns.

For instance, let's return to a DataFrame of orders from ShoeFly.com:
id 	first_name 	last_name 	email 	shoe_type 	shoe_material 	shoe_color
54791 	Rebecca 	Lindsay 	RebeccaLindsay57@hotmail.com 	clogs 	faux-leather 	black
53450 	Emily 	Joyce 	EmilyJoyce25@gmail.com 	ballet flats 	faux-leather 	navy
91987 	Joyce 	Waller 	Joyce.Waller@gmail.com 	sandals 	fabric 	black
14437 	Justin 	Erickson 	Justin.Erickson@outlook.com 	clogs 	faux-leather 	red

We might just be interested in the customer's last_name and email. We want a DataFrame like this:
last_name 	email
Lindsay 	RebeccaLindsay57@hotmail.com
Joyce 	EmilyJoyce25@gmail.com
Waller 	Joyce.Waller@gmail.com
Erickson 	Justin.Erickson@outlook.com

To select two or more columns from a DataFrame, we use a list of the column names. To create the DataFrame shown above, we would use:

new_df = orders[['last_name', 'email']]

*Note: *Make sure that you have a double set of brackets ([[]]), or this command won't work!

E.g.
import pandas as pd

df = pd.DataFrame([
  ['January', 100, 100, 23, 100],
  ['February', 51, 45, 145, 45],
  ['March', 81, 96, 65, 96],
  ['April', 80, 80, 54, 180],
  ['May', 51, 54, 54, 154],
  ['June', 112, 109, 79, 129]],
  columns=['month', 'clinic_east',
           'clinic_north', 'clinic_south',
           'clinic_west']
)

clinic_north_south = df[['clinic_north', 'clinic_south']]

print(type(clinic_north_south))

Select Rows

Let's revisit our orders from ShoeFly.com:
id 	first_name 	last_name 	email 	shoe_type 	shoe_material 	shoe_color
54791 	Rebecca 	Lindsay 	RebeccaLindsay57@hotmail.com 	clogs 	faux-leather 	black
53450 	Emily 	James 	EmilyJames25@gmail.com 	ballet flats 	faux-leather 	navy
91987 	Joyce 	Waller 	Joyce.Waller@gmail.com 	sandals 	fabric 	black
14437 	Justin 	Erickson 	Justin.Erickson@outlook.com 	clogs 	faux-leather 	red
... 						

Maybe our Customer Service department has just received a message from Joyce Waller, so we want to know exactly what she ordered. We want to select this single row of data.

DataFrames are zero-indexed, meaning that we start with the 0th row and count up from there. Joyce Waller's order is the 2nd row.

We select it using the following command:

orders.iloc[2]

When we select a single row, the result is a Series (just like when we select a single column).

Selecting Multiple Rows

You can also select multiple rows from a DataFrame.

Here are a few more rows from ShoeFly.com's orders DataFrame:
id 	first_name 	last_name 	email 	shoe_type 	shoe_material 	shoe_color
54791 	Rebecca 	Lindsay 	RebeccaLindsay57@hotmail.com 	clogs 	faux-leather 	black
53450 	Emily 	Joyce 	EmilyJoyce25@gmail.com 	ballet flats 	faux-leather 	navy
91987 	Joyce 	Waller 	Joyce.Waller@gmail.com 	sandals 	fabric 	black
14437 	Justin 	Erickson 	Justin.Erickson@outlook.com 	clogs 	faux-leather 	red
79357 	Andrew 	Banks 	AB4318@gmail.com 	boots 	leather 	brown
52386 	Julie 	Marsh 	JulieMarsh59@gmail.com 	sandals 	fabric 	black
20487 	Thomas 	Jensen 	TJ5470@gmail.com 	clogs 	fabric 	navy
76971 	Janice 	Hicks 	Janice.Hicks@gmail.com 	clogs 	faux-leather 	navy
21586 	Gabriel 	Porter 	GabrielPorter24@gmail.com 	clogs 	leather 	brown

Here are some different ways of selecting multiple rows:

    orders.iloc[3:7] would select all rows starting at the 3rd row and up to but not including the 7th row (i.e., the 3rd row, 4th row, 5th row, and 6th row)


id 	first_name 	last_name 	email 	shoe_type 	shoe_material 	shoe_color
14437 	Justin 	Erickson 	Justin.Erickson@outlook.com 	clogs 	faux-leather 	red
79357 	Andrew 	Banks 	AB4318@gmail.com 	boots 	leather 	brown
52386 	Julie 	Marsh 	JulieMarsh59@gmail.com 	sandals 	fabric 	black
20487 	Thomas 	Jensen 	TJ5470@gmail.com 	clogs 	fabric 	navy

    orders.iloc[:4] would select all rows up to, but not including the 4th row (i.e., the 0th, 1st, 2nd, and 3rd rows)


id 	first_name 	last_name 	email 	shoe_type 	shoe_material 	shoe_color
54791 	Rebecca 	Lindsay 	RebeccaLindsay57@hotmail.com 	clogs 	faux-leather 	black
53450 	Emily 	Joyce 	EmilyJoyce25@gmail.com 	ballet flats 	faux-leather 	navy
91987 	Joyce 	Waller 	Joyce.Waller@gmail.com 	sandals 	fabric 	black
14437 	Justin 	Erickson 	Justin.Erickson@outlook.com 	clogs 	faux-leather 	red

    orders.iloc[-3:] would select the rows starting at the 3rd to last row and up to and including the final row


id 	first_name 	last_name 	email 	shoe_type 	shoe_material 	shoe_color
20487 	Thomas 	Jensen 	TJ5470@gmail.com 	clogs 	fabric 	navy
76971 	Janice 	Hicks 	Janice.Hicks@gmail.com 	clogs 	faux-leather 	navy
21586 	Gabriel 	Porter 	GabrielPorter24@gmail.com 	clogs leather brown

Select Rows with Logic I

You can select a subset of a DataFrame by using logical statements:

df[df.MyColumnName == desired_column_value]

We have a large DataFrame with information about our customers. A few of the many rows look like this:
name 	address 	phone 	age
Martha Jones 	123 Main St. 	234-567-8910 	28
Rose Tyler 	456 Maple Ave. 	212-867-5309 	22
Donna Noble 	789 Broadway 	949-123-4567 	35
Amy Pond 	98 West End Ave. 	646-555-1234 	29
Clara Oswald 	54 Columbus Ave. 	714-225-1957 	31
... 	... 	... 	...

Suppose we want to select all rows where the customer's age is 30. We would use:

df[df.age == 30]

In Python, == is how we test if a value is exactly equal to another value.

We can use other logical statements, such as:

    Greater Than, > — Here, we select all rows where the customer's age is greater than 30:

    df[df.age > 30]

    Less Than, < — Here, we select all rows where the customer's age is less than 30:

    df[df.age < 30]

    Not Equal, != — This snippet selects all rows where the customer's name is not Clara Oswald:

    df[df.name != 'Clara Oswald']


Select Rows with Logic II

You can also combine multiple logical statements, as long as each statement is in parentheses.

For instance, suppose we wanted to select all rows where the customer's age was under 30 or the customer's name was "Martha Jones":
name 	address 	phone 	age
Martha Jones 	123 Main St. 	234-567-8910 	28
Rose Tyler 	456 Maple Ave. 	212-867-5309 	22
Donna Noble 	789 Broadway 	949-123-4567 	35
Amy Pond 	98 West End Ave. 	646-555-1234 	29
Clara Oswald 	54 Columbus Ave. 	714-225-1957 	31
... 			

We could use the following code:

df[(df.age < 30) |
   (df.name == 'Martha Jones')]

In Python, | means "or" and & means "and".


Select Rows with Logic III

Suppose we want to select the rows where the customer's name is either "Martha Jones", "Rose Tyler" or "Amy Pond".
name 	address 	phone 	age
Martha Jones 	123 Main St. 	234-567-8910 	28
Rose Tyler 	456 Maple Ave. 	212-867-5309 	22
Donna Noble 	789 Broadway 	949-123-4567 	35
Amy Pond 	98 West End Ave. 	646-555-1234 	29
Clara Oswald 	54 Columbus Ave. 	714-225-1957 	31
... 	... 	... 	...

We could use the isin command to check that df.name is one of a list of values:

df[df.name.isin(['Martha Jones',
     'Rose Tyler',
     'Amy Pond'])]

Setting indices

When we select a subset of a DataFrame using logic, we end up with non-consecutive indices. This is inelegant and makes it hard to use .iloc().

We can fix this using the method .reset_index(). For example, here is a DataFrame called df with non-consecutive indices:
	First Name 	Last Name
0 	John 	Smith
4 	Jane 	Doe
7 	Joe 	Schmo

If we use the command df.reset_index(), we get a new DataFrame with a new set of indices:
	index 	First Name 	Last Name
0 	0 	John 	Smith
1 	4 	Jane 	Doe
2 	7 	Joe 	Schmo

Note that the old indices have been moved into a new column called 'index'. Unless you need those values for something special, it's probably better to use the keyword drop=True so that you don't end up with that extra column. If we run the command df.reset_index(drop=True), we get a new DataFrame that looks like this:
	First Name 	Last Name
0 	John 	Smith
1 	Jane 	Doe
2 	Joe 	Schmo

Using .reset_index() will return a new DataFrame, but we usually just want to modify our existing DataFrame. If we use the keyword inplace=True we can just modify our existing DataFrame.


Review

You've completed the lesson! You've just learned the basics of working with a single table in Pandas, including:

    Create a table from scratch
    Loading data from another file
    Selecting certain rows or columns of a table

Let's practice what you've learned.


\subsection{Modifying Dataframes}
Modifying DataFrames

In the previous lesson, you learned what a DataFrame is and how to select subsets of data from one.

In this lesson, you'll learn how to modify an existing DataFrame. Some of the skills you'll learn include:

    Adding columns to a DataFrame
    Using lambda functions to calculate complex quantities
    Renaming columns

Adding a Column I

Sometimes, we want to add a column to an existing DataFrame. We might want to add new information or perform a calculation based on the data that we already have.

One way that we can add a new column is by giving a list of the same length as the existing DataFrame.

Suppose we own a hardware store called The Handy Woman and have a DataFrame containing inventory information:
Product ID 	Product Description 	Cost to Manufacture 	Price
1 	3 inch screw 	0.50 	0.75
2 	2 inch nail 	0.10 	0.25
3 	hammer 	3.00 	5.50
4 	screwdriver 	2.50 	3.00

It looks like the actual quantity of each product in our warehouse is missing!

Let's use the following code to add that information to our DataFrame.

df['Quantity'] = [100, 150, 50, 35]

Our new DataFrame looks like this:
Product ID 	Product Description 	Cost to Manufacture 	Price 	Quantity
1 	3 inch screw 	0.50 	0.75 	100
2 	2 inch nail 	0.10 	0.25 	150
3 	hammer 	3.00 	5.50 	50
4 	screwdriver 	2.50 	3.00 	35


Adding a Column II

We can also add a new column that is the same for all rows in the DataFrame. Let's return to our inventory example:
Product ID 	Product Description 	Cost to Manufacture 	Price
1 	3 inch screw 	0.50 	0.75
2 	2 inch nail 	0.10 	0.25
3 	hammer 	3.00 	5.50
4 	screwdriver 	2.50 	3.00

Suppose we know that all of our products are currently in-stock. We can add a column that says this:

df['In Stock?'] = True

Now all of the rows have a column called In Stock? with value True.
Product ID 	Product Description 	Cost to Manufacture 	Price 	In Stock?
1 	3 inch screw 	0.50 	0.75 	True
2 	2 inch nail 	0.10 	0.25 	True
3 	hammer 	3.00 	5.50 	True
4 	screwdriver 	2.50 	3.00 	True



Adding a Column III

Finally, you can add a new column by performing a function on the existing columns.

Maybe we want to add a column to our inventory table with the amount of sales tax that we need to charge for each item. The following code multiplies each Price by 0.075, the sales tax for our state:

df['Sales Tax'] = df.Price * 0.075

Now our table has a column called Sales Tax:
Product ID 	Product Description 	Cost to Manufacture 	Price 	Sales Tax
1 	3 inch screw 	0.50 	0.75 	0.06
2 	2 inch nail 	0.10 	0.25 	0.02
3 	hammer 	3.00 	5.50 	0.41
4 	screwdriver 	2.50 	3.00 	0.22


Performing Column Operations

In the previous exercise, we learned how to add columns to a DataFrame.

Often, the column that we want to add is related to existing columns, but requires a calculation more complex than multiplication or addition.

For example, imagine that we have the following table of customers.
Name 	Email
JOHN SMITH 	john.smith@gmail.com
Jane Doe 	jdoe@yahoo.com
joe schmo 	joeschmo@hotmail.com

It’s a little annoying that the capitalization is different for each row. Perhaps we’d like to make it more consistent by making all of the letters uppercase.

We can use the apply function to apply a function to every value in a particular column. For example, this code overwrites the existing 'Name' columns by applying the function upper to every row in 'Name'.

from string import upper

df['Name'] = df.Name.apply(upper)

The result:
Name 	Email
JOHN SMITH 	john.smith@gmail.com
JANE DOE 	jdoe@yahoo.com
JOE SCHMO 	joeschmo@hotmail.com


Reviewing Lambda Function

A lambda function is a way of defining a function in a single line of code. Usually, we would assign them to a variable.

For example, the following lambda function multiplies a number by 2 and then adds 3:

mylambda = lambda x: (x * 2) + 3
print(mylambda(5))

The output:

> 13

Lambda functions work with all types of variables, not just integers! Here is an example that takes in a string, assigns it to the temporary variable x, and then converts it into lowercase:

stringlambda = lambda x: x.lower()
print(stringlambda("Oh Hi Mark!"))

The output:

> "oh hi mark!"

Learn more about lambda functions in this article!


Reviewing Lambda Function: If Statements

We can make our lambdas more complex by using a modified form of an if statement.

Suppose we want to pay workers time-and-a-half for overtime (any work above 40 hours per week). The following function will convert the number of hours into time-and-a-half hours using an if statement:

def myfunction(x):
    if x > 40:
        return 40 + (x - 40) * 1.50
    else:
        return x

Below is a lambda function that does the same thing:

myfunction = lambda x: 40 + (x - 40) * 1.50 \
    if x > 40 else x

In general, the syntax for an if function in a lambda function is:

lambda x: [OUTCOME IF TRUE] \
    if [CONDITIONAL] \
    else [OUTCOME IF FALSE]

Applying a Lambda to a Column

In Pandas, we often use lambda functions to perform complex operations on columns. For example, suppose that we want to create a column containing the email provider for each email address in the following table:
Name 	Email
JOHN SMITH 	john.smith@gmail.com
Jane Doe 	jdoe@yahoo.com
joe schmo 	joeschmo@hotmail.com

We could use the following code with a lambda function:

df['Email Provider'] = df.Email.apply(
    lambda x: x.split('@')[-1]
    )

The result would be:
Name 	Email 	Email Provider
JOHN SMITH 	john.smith@gmail.com 	gmail.com
Jane Doe 	jdoe@yahoo.com 	yahoo.com
joe schmo 	joeschmo@hotmail.com 	hotmail.com


Applying a Lambda to a Row

We can also operate on multiple columns at once. If we use apply without specifying a single column and add the argument axis=1, the input to our lambda function will be an entire row, not a column. To access particular values of the row, we use the syntax row.column_name or row[‘column_name’].

Suppose we have a table representing a grocery list:
Item 	Price 	Is taxed?
Apple 	1.00 	No
Milk 	4.20 	No
Paper Towels 	5.00 	Yes
Light Bulbs 	3.75 	Yes

If we want to add in the price with tax for each line, we’ll need to look at two columns: Price and Is taxed?.

If Is taxed? is Yes, then we’ll want to multiply Price by 1.075 (for 7.5% sales tax).

If Is taxed? is No, we’ll just have Price without multiplying it.

We can create this column using a lambda function and the keyword axis=1:

df['Price with Tax'] = df.apply(lambda row:
     row['Price'] * 1.075
     if row['Is taxed?'] == 'Yes'
     else row['Price'],
     axis=1
)

Renaming Columns

When we get our data from other sources, we often want to change the column names. For example, we might want all of the column names to follow variable name rules, so that we can use df.column_name (which tab-completes) rather than df['column_name'] (which takes up extra space).

You can change all of the column names at once by setting the .columns property to a different list. This is great when you need to change all of the column names at once, but be careful! You can easily mislabel columns if you get the ordering wrong. Here's an example:

df = pd.DataFrame({
    'name': ['John', 'Jane', 'Sue', 'Fred'],
    'age': [23, 29, 21, 18]
})
df.columns = ['First Name', 'Age']

This command edits the existing DataFrame df.


Renaming Columns II

You also can rename individual columns by using the .rename method. Pass a dictionary like the one below to the columns keyword argument:

{'old_column_name1': 'new_column_name1', 'old_column_name2': 'new_column_name2'}

Here's an example:

df = pd.DataFrame({
    'name': ['John', 'Jane', 'Sue', 'Fred'],
    'age': [23, 29, 21, 18]
})
df.rename(columns={
    'name': 'First Name',
    'age': 'Age'},
    inplace=True)

The code above will rename name to First Name and age to Age.

Using rename with only the columns keyword will create a new DataFrame, leaving your original DataFrame unchanged. That's why we also passed in the keyword argument inplace=True. Using inplace=True lets us edit the original DataFrame.

There are several reasons why .rename is preferable to .columns:

    You can rename just one column
    You can be specific about which column names are getting changed (with .column you can accidentally switch column names if you're not careful)

*Note: *If you misspell one of the original column names, this command won't fail. It just won't change anything.

Review

Great job! In this lesson, you learned how to modify an existing DataFrame. Some of the skills you've learned include:

    Adding columns to a DataFrame
    Using lambda functions to calculate complex quantities
    Renaming columns

Let's practice what you just learned!


example project

import pandas as pd

inventory = pd.read_csv('inventory.csv')

print(inventory.head(10))

staten_island = inventory.head(10)

product_request = staten_island['product_description']
seed_request = staten_island[staten_island.product_type == 'seeds']

print(product_request)
print(seed_request)

inventory["in_stock"] = inventory.quantity > 0
inventory["total_value"] = inventory.price * inventory.quantity
combine_lambda = lambda row: '{} - {}'.format(row.product_type, row.product_description)

inventory['full_description'] = inventory.apply(combine_lambda, axis=1)

print(inventory.head())

\subsection{Aggregates in Pandas}
Introduction

This lesson you will learn about aggregates in Pandas. An aggregate statistic is a way of creating a single number that describes a group of numbers. Common aggregate statistics incluse mean, median, or standard deviation.

You will also learn how to rearrange a DataFrame into a pivot table, which is a great way to compare data across two dimensions.



Calculating Column Statistics

In the previous lesson, you learned how to perform operations on each value in a column using apply.

In this exercise, you will learn how to combine all of the values from a column for a single calculation.

Some examples of this type of calculation include:

    The DataFrame customers contains the names and ages of all of your customers. You want to find the median age:

    print(customers.age)
    >> [23, 25, 31, 35, 35, 46, 62]
    print(customers.age.median())
    >> 35

    The DataFrame shipments contains address information for all shipments that you've sent out in the past year. You want to know how many different states you have shipped to (and how many shipments went to the same state).

    print(shipments.state)
    >> ['CA', 'CA', 'CA', 'CA', 'NY', 'NY', 'NJ', 'NJ', 'NJ', 'NJ', 'NJ', 'NJ', 'NJ']
    print(shipments.state.nunique())
    >> 3

    The DataFrame inventory contains a list of types of t-shirts that your company makes. You want a list of the colors that your shirts come in.

    print(inventory.color)
    >> ['blue', 'blue', 'blue', 'blue', 'blue', 'green', 'green', 'orange', 'orange', 'orange']
    print(inventory.color.unique())
    >> ['blue', 'green', 'orange']

The general syntax for these calculations is:

df.column_name.command()

The following table summarizes some common commands:
Command 	Description
mean 	Average of all values in column
std 	Standard deviation
median 	Median
max 	Maximum value in column
min 	Minimum value in column
count 	Number of values in column
nunique 	Number of unique values in column
unique 	List of unique values in column



Calculating Aggregate Functions I

When we have a bunch of data, we often want to calculate aggregate statistics (mean, standard deviation, median, percentiles, etc.) over certain subsets of the data.

Suppose we have a grade book with columns student, assignment_name, and grade. The first few lines look like this:
student 	assignment_name 	grade
Amy 	Assignment 1 	75
Amy 	Assignment 2 	35
Bob 	Assignment 1 	99
Bob 	Assignment 2 	35
... 		
	

We want to get an average grade for each student across all assignments. We could do some sort of loop, but Pandas gives us a much easier option: the method .groupby.

For this example, we'd use the following command:

grades = df.groupby('student').grade.mean()

The output might look something like this:
student 	grade
Amy 	80
Bob 	90
Chris 	75
... 	
	

In general, we use the following syntax to calculate aggregates:

df.groupby('column1').column2.measurement()

where:

    column1 is the column that we want to group by ('student' in our example)
    column2 is the column that we want to perform a measurement on (grade in our example)
    measurement is the measurement function we want to apply (mean in our example)


Calculating Aggregate Functions II

After using groupby, we often need to clean our resulting data.

As we saw in the previous exercise, the groupby function creates a new Series, not a DataFrame. For our ShoeFly.com example, the indices of the Series were different values of shoe_type, and the name property was price.

Usually, we'd prefer that those indices were actually a column. In order to get that, we can use reset_index(). This will transform our Series into a DataFrame and move the indices into their own column.

Generally, you'll always see a groupby statement followed by reset_index:

df.groupby('column1').column2.measurement()
    .reset_index()

When we use groupby, we often want to rename the column we get as a result. For example, suppose we have a DataFrame teas containing data on types of tea:
id 	tea 	category 	caffeine 	price
0 	earl grey 	black 	38 	3
1 	english breakfast 	black 	41 	3
2 	irish breakfast 	black 	37 	2.5
3 	jasmine 	green 	23 	4.5
4 	matcha 	green 	48 	5
5 	camomile 	herbal 	0 	3
... 				

We want to find the number of each category of tea we sell. We can use:

teas_counts = teas.groupby('category').id.count().reset_index()

This yields a DataFrame that looks like:
	category 	id
0 	black 	3
1 	green 	4
2 	herbal 	8
3 	white 	2
... 		
	

The new column contains the counts of each category of tea sold. We have 3 black teas, 4 green teas, and so on. However, this column is called id because we used the id column of teas to calculate the counts. We actually want to call this column counts. Remember that we can rename columns:

teas_counts = teas_counts.rename(columns={"id": "counts"})

Our DataFrame now looks like:
	category 	counts
0 	black 	3
1 	green 	4
2 	herbal 	8
3 	white 	2
... 		
		
		
Calculating Aggregate Functions III

Sometimes, the operation that you want to perform is more complicated than mean or count. In those cases, you can use the apply method and lambda functions, just like we did for individual column operations. Note that the input to our lambda function will always be a list of values.

A great example of this is calculating percentiles. Suppose we have a DataFrame of employee information called df that has the following columns:

    id: the employee's id number
    name: the employee's name
    wage: the employee's hourly wage
    category: the type of work that the employee does

Our data might look something like this:
id 	name 	wage 	category
10131 	Sarah Carney 	39 	product
14189 	Heather Carey 	17 	design
15004 	Gary Mercado 	33 	marketing
11204 	Cora Copaz 	27 	design
... 			

If we want to calculate the 75th percentile (i.e., the point at which 75% of employees have a lower wage and 25% have a higher wage) for each category, we can use the following combination of apply and a lambda function:

# np.percentile can calculate any percentile over an array of values
high_earners = df.groupby('category').wage
    .apply(lambda x: np.percentile(x, 75))
    .reset_index()

The output, high_earners might look like this:
	category 	wage
0 	design 	23
1 	marketing 	35
2 	product 	48
... 		

Calculating Aggregate Functions IV

Sometimes, we want to group by more than one column. We can easily do this by passing a list of column names into the groupby method.

Imagine that we run a chain of stores and have data about the number of sales at different locations on different days:
Location 	Date 	Day of Week 	Total Sales
West Village 	February 1 	W 	400
West Village 	February 2 	Th 	450
Chelsea 	February 1 	W 	375
Chelsea 	February 2 	Th 	390
		

We suspect that sales are different at different locations on different days of the week. In order to test this hypothesis, we could calculate the average sales for each store on each day of the week across multiple months. The code would look like this:

df.groupby(['Location', 'Day of Week'])['Total Sales'].mean().reset_index()

The results might look something like this:
Location 	Day of Week 	Total Sales
Chelsea 	M 	402.50
Chelsea 	Tu 	422.75
Chelsea 	W 	452.00
... 		
West Village 	M 	390
West Village 	Tu 	400
... 		
		
		

Pivot Tables

When we perform a groupby across multiple columns, we often want to change how our data is stored. For instance, recall the example where we are running a chain of stores and have data about the number of sales at different locations on different days:
Location 	Date 	Day of Week 	Total Sales
West Village 	February 1 	W 	400
West Village 	February 2 	Th 	450
Chelsea 	February 1 	W 	375
Chelsea 	February 2 	Th 	390
		We suspected that there might be different sales on different days of the week at different stores, so we performed a groupby across two different columns (Location and Day of Week). This gave us results that looked like this: 			
Location 	Day of Week 	Total Sales
Chelsea 	M 	300
Chelsea 	Tu 	310
Chelsea 	W 	320
Chelsea 	Th 	290
... 		
West Village 	Th 	400
West Village 	F 	390
West Village 	Sa 	250
... 		
		In order to test our hypothesis, it would be more useful if the table was formatted like this: 		
Location 	M 	Tu 	W 	Th 	F 	Sa 	Su
Chelsea 	400 	390 	250 	275 	300 	150 	175
West Village 	300 	310 	350 	400 	390 	250 	200
... 							

Reorganizing a table in this way is called pivoting. The new table is called a pivot table.

In Pandas, the command for pivot is:

df.pivot(columns='ColumnToPivot',
         index='ColumnToBeRows',
         values='ColumnToBeValues')

For our specific example, we would write the command like this:

# First use the groupby statement:
unpivoted = df.groupby(['Location', 'Day of Week'])['Total Sales'].mean().reset_index()
# Now pivot the table
pivoted = unpivoted.pivot(
    columns='Day of Week',
    index='Location',
    values='Total Sales')

Just like with groupby, the output of a pivot command is a new DataFrame, but the indexing tends to be "weird", so we usually follow up with .reset_index().

Review

This lesson introduced you to aggregates in Pandas. You learned:

    How to perform aggregate statistics over individual rows with the same value using groupby.
    How to rearrange a DataFrame into a pivot table, a great way to compare data across two dimensions.

import codecademylib
import pandas as pd

user_visits = pd.read_csv('page_visits.csv')

print(user_visits.head())

click_source = user_visits.groupby(["utm_source"]).id.count().reset_index()

print(click_source)

click_source_by_month = user_visits.groupby(["utm_source", "month"]).id.count().reset_index()

print(click_source_by_month)

click_source_by_month_pivot = click_source_by_month.pivot(
  columns="month",
  index="utm_source",
  values="id").reset_index()

print(click_source_by_month_pivot)

\subsection{Multiple Dataframes}
Introduction: Multiple DataFrames

In order to efficiently store data, we often spread related information across multiple tables.

For instance, imagine that we own an e-commerce business and we want to track the products that have been ordered from our website.

We could have one table with all of the following information:

    order_id
    customer_id
    customer_name
    customer_address
    customer_phone_number
    product_id
    product_description
    product_price
    quantity
    timestamp

However, a lot of this information would be repeated. If the same customer makes multiple orders, that customer’s name, address, and phone number will be reported multiple times. If the same product is ordered by multiple customers, then the product price and description will be repeated. This will make our orders table big and unmanageable.

So instead, we can split our data into three tables:

    orders would contain the information necessary to describe an order: order_id, customer_id, product_id, quantity, and timestamp
    products would contain the information to describe each product: product_id, product_description and product_price
    customers would contain the information for each customer: customer_id, customer_name, customer_address, and customer_phone_number

In this lesson, we will learn the Pandas commands that help us work with data stored in multiple tables.


Suppose we have the following three tables that describe our eCommerce business:

    orders — a table with information on each transaction:

order_id	customer_id	product_id	quantity	timestamp
1	2	3	1	2017-01-01
2	2	2	3	2017-01-01
3	3	1	1	2017-01-01
4	3	2	2	2017-02-01
5	3	3	3	2017-02-01
6	1	4	2	2017-03-01
7	1	1	1	2017-02-02
8	1	4	1	2017-02-02

    products — a table with product IDs, descriptions, and prices:

product_id	description	price
1	thing-a-ma-jig	5
2	whatcha-ma-call-it	10
3	doo-hickey	7
4	gizmo	3

    customers — a table with customer names and contact information:

customer_id	customer_name	address	phone_number
1	John Smith	123 Main St.	212-123-4567
2	Jane Doe	456 Park Ave.	949-867-5309
3	Joe Schmo	798 Broadway	112-358-1321

If we just look at the orders table, we can’t really tell what’s happened in each order. However, if we refer to the other tables, we can get a more complete picture.

Let’s examine the order with an order_id of 1. It was purchased by Customer 2. To find out the customer’s name, we look at the customers table and look for the item with a customer_id value of 2. We can see that Customer 2’s name is Jane Doe and that she lives at 456 Park Ave.

Doing this kind of matching is called merging two DataFrames.


Inner Merge II

It is easy to do this kind of matching for one row, but hard to do it for multiple rows.

Luckily, Pandas can efficiently do this for the entire table. We use the .merge method.

The .merge method looks for columns that are common between two DataFrames and then looks for rows where those column’s values are the same. It then combines the matching rows into a single row in a new table.

We can call the pd.merge method with two tables like this:

new_df = pd.merge(orders, customers)

This will match up all of the customer information to the orders that each customer made.

In addition to using pd.merge, each DataFrame has its own merge method. For instance, if you wanted to merge orders with customers, you could use:

new_df = orders.merge(customers)

This produces the same DataFrame as if we had called pd.merge(orders, customers).

We generally use this when we are joining more than two DataFrames together because we can “chain” the commands. The following command would merge orders to customers, and then the resulting DataFrame to products:

big_df = orders.merge(customers)\
    .merge(products)


Merge on Specific Columns

In the previous example, the merge function “knew” how to combine tables based on the columns that were the same between two tables. For instance, products and orders both had a column called product_id. This won’t always be true when we want to perform a merge.

Generally, the products and customers DataFrames would not have the columns product_id or customer_id. Instead, they would both be called id and it would be implied that the id was the product_id for the products table and customer_id for the customers table. They would look like this:
Customers
id	customer_name	address	phone_number
1	John Smith	123 Main St.	212-123-4567
2	Jane Doe	456 Park Ave.	949-867-5309
3	Joe Schmo	798 Broadway	112-358-1321
Products
id	description	price
1	thing-a-ma-jig	5
2	whatcha-ma-call-it	10
3	doo-hickey	7
4	gizmo	3

**How would this affect our merges?**

Because the id columns would mean something different in each table, our default merges would be wrong.

One way that we could address this problem is to use .rename to rename the columns for our merges. In the example below, we will rename the column id to customer_id, so that orders and customers have a common column for the merge.

pd.merge(
    orders,
    customers.rename(columns={'id': 'customer_id'}))

Merge on Specific Columns II

In the previous exercise, we learned how to use rename to merge two DataFrames whose columns don’t match.

If we don’t want to do that, we have another option. We could use the keywords left_on and right_on to specify which columns we want to perform the merge on. In the example below, the “left” table is the one that comes first (orders), and the “right” table is the one that comes second (customers). This syntax says that we should match the customer_id from orders to the id in customers.

pd.merge(
    orders,
    customers,
    left_on='customer_id',
    right_on='id')

If we use this syntax, we’ll end up with two columns called id, one from the first table and one from the second. Pandas won’t let you have two columns with the same name, so it will change them to id_x and id_y.

It will look like this:
id_x 	customer_id 	product_id 	quantity 	timestamp 	id_y 	customer_name 	address 	phone_number
1 	2 	3 	1 	2017-01-01 00:00:00 	2 	Jane Doe 	456 Park Ave 	949-867-5309
2 	2 	2 	3 	2017-01-01 00:00:00 	2 	Jane Doe 	456 Park Ave 	949-867-5309
3 	3 	1 	1 	2017-01-01 00:00:00 	3 	Joe Schmo 	789 Broadway 	112-358-1321
4 	3 	2 	2 	2016-02-01 00:00:00 	3 	Joe Schmo 	789 Broadway 	112-358-1321
5 	3 	3 	3 	2017-02-01 00:00:00 	3 	Joe Schmo 	789 Broadway 	112-358-1321
6 	1 	4 	2 	2017-03-01 00:00:00 	1 	John Smith 	123 Main St. 	212-123-4567
7 	1 	1 	1 	2017-02-02 00:00:00 	1 	John Smith 	123 Main St. 	212-123-4567
8 	1 	4 	1 	2017-02-02 00:00:00 	1 	John Smith 	123 Main St. 	212-123-4567

The new column names id_x and id_y aren’t very helpful for us when we read the table. We can help make them more useful by using the keyword suffixes. We can provide a list of suffixes to use instead of “_x” and “_y”.

For example, we could use the following code to make the suffixes reflect the table names:

pd.merge(
    orders,
    customers,
    left_on='customer_id',
    right_on='id',
    suffixes=['_order', '_customer']
)

The resulting table would look like this:
id_order 	customer_id 	product_id 	quantity 	timestamp 	id_customer 	customer_name 	address 	phone_number
1 	2 	3 	1 	2017-01-01 00:00:00 	2 	Jane Doe 	456 Park Ave 	949-867-5309
2 	2 	2 	3 	2017-01-01 00:00:00 	2 	Jane Doe 	456 Park Ave 	949-867-5309
3 	3 	1 	1 	2017-01-01 00:00:00 	3 	Joe Schmo 	789 Broadway 	112-358-1321
4 	3 	2 	2 	2016-02-01 00:00:00 	3 	Joe Schmo 	789 Broadway 	112-358-1321
5 	3 	3 	3 	2017-02-01 00:00:00 	3 	Joe Schmo 	789 Broadway 	112-358-1321
6 	1 	4 	2 	2017-03-01 00:00:00 	1 	John Smith 	123 Main St. 	212-123-4567
7 	1 	1 	1 	2017-02-02 00:00:00 	1 	John Smith 	123 Main St. 	212-123-4567
8 	1 	4 	1 	2017-02-02 00:00:00 	1 	John Smith 	123 Main St. 	212-123-4567
							
Mismatched Merges

In our previous examples, there were always matching values when we were performing our merges. What happens when that isn’t true?

Let’s imagine that our products table is out of date and is missing the newest product: Product 5. What happens when someone orders it?

Outer Merge

In the previous exercise, we saw that when we merge two DataFrames whose rows don’t match perfectly, we lose the unmatched rows.

This type of merge (where we only include matching rows) is called an inner merge. There are other types of merges that we can use when we want to keep information from the unmatched rows.

Suppose that two companies, Company A and Company B have just merged. They each have a list of customers, but they keep slightly different data. Company A has each customer’s name and email. Company B has each customer’s name and phone number. They have some customers in common, but some are different.

company_a
name 	email
Sally Sparrow 	sally.sparrow@gmail.com
Peter Grant 	pgrant@yahoo.com
Leslie May 	leslie_may@gmail.com

company_b
name 	phone
Peter Grant 	212-345-6789
Leslie May 	626-987-6543
Aaron Burr 	303-456-7891

If we wanted to combine the data from both companies without losing the customers who are missing from one of the tables, we could use an Outer Join. An Outer Join would include all rows from both tables, even if they don’t match. Any missing values are filled in with None or nan (which stands for “Not a Number”).

pd.merge(company_a, company_b, how='outer')

The resulting table would look like this:
name 	email 	phone
Sally Sparrow 	sally.sparrow@gmail.com 	nan
Peter Grant 	pgrant@yahoo.com 	212-345-6789
Leslie May 	leslie_may@gmail.com 	626-987-6543
Aaron Burr 	nan 	303-456-7891


Left and Right Merge

Let’s return to the merge of Company A and Company B.
Left Merge

Suppose we want to identify which customers are missing phone information. We would want a list of all customers who have email, but don’t have phone.

We could get this by performing a Left Merge. A Left Merge includes all rows from the first (left) table, but only rows from the second (right) table that match the first table.

For this command, the order of the arguments matters. If the first DataFrame is company_a and we do a left join, we’ll only end up with rows that appear in company_a.

By listing company_a first, we get all customers from Company A, and only customers from Company B who are also customers of Company A.

pd.merge(company_a, company_b, how='left')

The result would look like this:
name 	email 	phone
Sally Sparrow 	sally.sparrow@gmail.com 	None
Peter Grant 	pgrant@yahoo.com 	212-345-6789
Leslie May 	leslie_may@gmail.com 	626-987-6543

Now let’s say we want a list of all customers who have phone but no email. We can do this by performing a Right Merge.
Right Merge

Right merge is the exact opposite of left merge. Here, the merged table will include all rows from the second (right) table, but only rows from the first (left) table that match the second table.

By listing company_a first and company_b second, we get all customers from Company B, and only customers from Company A who are also customers of Company B.

pd.merge(company_a, company_b, how="right")

The result would look like this:
name 	email 	phone
Peter Grant 	pgrant@yahoo.com 	212-345-6789
Leslie May 	leslie_may@gmail.com 	626-987-6543
Aaron Burr 	None 	303-456-7891


Concatenate DataFrames

Sometimes, a dataset is broken into multiple tables. For instance, data is often split into multiple CSV files so that each download is smaller.

When we need to reconstruct a single DataFrame from multiple smaller DataFrames, we can use the method pd.concat([df1, df2, df2, ...]). This method only works if all of the columns are the same in all of the DataFrames.

For instance, suppose that we have two DataFrames:
df1
name	email
Katja Obinger	k.obinger@gmail.com
Alison Hendrix	alisonH@yahoo.com
Cosima Niehaus	cosi.niehaus@gmail.com
Rachel Duncan	rachelduncan@hotmail.com
df2
name	email
Jean Gray	jgray@netscape.net
Scott Summers	ssummers@gmail.com
Kitty Pryde	kitkat@gmail.com
Charles Xavier	cxavier@hotmail.com

If we want to combine these two DataFrames, we can use the following command:

pd.concat([df1, df2])

That would result in the following DataFrame:
name	email
Katja Obinger	k.obinger@gmail.com
Alison Hendrix	alisonH@yahoo.com
Cosima Niehaus	cosi.niehaus@gmail.com
Rachel Duncan	rachelduncan@hotmail.com
Jean Gray	jgray@netscape.net

Review

This lesson introduced some methods for combining multiple DataFrames:

    Creating a DataFrame made by matching the common columns of two DataFrames is called a merge
    We can specify which columns should be matches by using the keyword arguments left_on and right_on
    We can combine DataFrames whose rows don’t all match using left, right, and outer merges and the how keyword argument
    We can stack or concatenate DataFrames with the same columns using pd.concat

\section{Matplotlib}
Matplotlib is a Python library used to create charts and graphs.

In this first lesson, you will get an overview of the basic commands necessary to build and label a line graph. The concepts you will learn include:

    Creating a line graph from data
    Changing the appearance of the line
    Zooming in on different parts of the axis
    Putting labels on titles and axes
    Creating a more complex figure layout
    Adding legends to graphs
    Changing tick labels and positions
    Saving what you’ve made

By the end of the lesson, you’ll be well on your way to becoming a master at presenting and organizing your data in Python.

To the right, you can see an example of the kind of chart you’ll be able to make by the end of this lesson!


ne graphs are helpful for visualizing how a variable changes over time.

Some possible data that would be displayed with a line graph:

    average prices of gasoline over the past decade
    weight of an individual over the past couple of months
    average temperature along a line of longitude over different latitudes

Using Matplotlib methods, the following code will create a simple line graph using .plot() and display it using .show() :

x_values = [0, 1, 2, 3, 4]
y_values = [0, 1, 4, 9, 16]
plt.plot(x_values, y_values)
plt.show()

    x_values is a variable holding a list of x-values for each point on our line graph
    y_values is a variable holding a list of y-values for each point on our line graph
    plt is the name we have given to the Matplotlib module we have imported at the top of the code
    plt.plot(x_values, y_values) will create the line graph
    plt.show() will actually display the graph

Our graph would look like this:

Let’s get some practice with plotting lines.


Basic Line Plot II

We can also have multiple line plots displayed on the same set of axes. This can be very useful if we want to compare two datasets with the same scale and axis categories.

Matplotlib will automatically place the two lines on the same axes and give them different colors if you call plt.plot() twice.

Let’s look at the graph we made in the last exercise to track lunch spending, where days is on the x-axis and spending (money_spent) is on the y-axis:

money_spent

We could add a friend’s lunch spending for comparison like this:

# Days of the week:
days = [0, 1, 2, 3, 4, 5, 6]
# Your Money:
money_spent = [10, 12, 12, 10, 14, 22, 24]
# Your Friend's Money:
money_spent_2 = [11, 14, 15, 15, 22, 21, 12]
# Plot your money:
plt.plot(days, money_spent)
# Plot your friend's money:
plt.plot(days, money_spent_2)
# Display the result:
plt.show()

We then get two lines on the same plot:

money_spent_2

By default, the first line is always blue, and the second line is always orange. In the next exercise, we’ll learn how to customize these lines ourselves.


Linestyles

We can specify a different color for a line by using the keyword color with either an HTML color name or a HEX code:

plt.plot(days, money_spent, color='green')
plt.plot(days, money_spent_2, color='#AAAAAA')

money_colors

We can also change make a line dotted or dashed using the keyword linestyle.

# Dashed:
plt.plot(x_values, y_values, linestyle='--')
# Dotted:
plt.plot(x_values, y_values, linestyle=':')
# No line:
plt.plot(x_values, y_values, linestyle='')

We can also add a marker using the keyword marker:

# A circle:
plt.plot(x_values, y_values, marker='o')
# A square:
plt.plot(x_values, y_values, marker='s')
# A star:
plt.plot(x_values, y_values, marker='*')

To see all of the possible options, check out the Matplotlib documentation. Here are a couple of those values applied to our plots about lunch spending:

plt.plot(days, money_spent, color='green', linestyle='--')
plt.plot(days, money_spent_2, color='#AAAAAA',  marker='o')

linestyles

Let’s get some practice with customizing lines on the same plot.



Axis and Labels

Sometimes, it can be helpful to zoom in or out of the plot, especially if there is some detail we want to address. To zoom, we can use plt.axis(). We use plt.axis() by feeding it a list as input. This list should contain:

    The minimum x-value displayed
    The maximum x-value displayed
    The minimum y-value displayed
    The maximum y-value displayed

For example, if we want to display a plot from x=0 to x=3 and from y=2 to y=5, we would call plt.axis([0, 3, 2, 5]).

x = [0, 1, 2, 3, 4]
y = [0, 1, 4, 9, 16]
plt.plot(x, y)
plt.axis([0, 3, 2, 5])
plt.show()



Labeling the Axes

Eventually, we will want to show these plots to other people to convince them of important trends in our data. When we do that, we’ll want to make our plots look as professional as possible.

The first step towards a professional-looking plot is adding labels to the x-axis and y-axis, and giving the plot a title.

We can label the x- and y- axes by using plt.xlabel() and plt.ylabel(). The plot title can be set by using plt.title().

All of these commands require a string, which is a set of characters in either single (') or double (") quotes.

"This is a string"
'This is also a string'

'This is NOT a string (the quotes do not match)"

For example, if someone has been keeping track of their happiness (on a scale out of 10) throughout the day and wants to display this information with labeled axes, we can use the following commands:

hours = [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
happiness = [9.8, 9.9, 9.2, 8.6, 8.3, 9.0, 8.7, 9.1, 7.0, 6.4, 6.9, 7.5]
plt.plot(hours, happiness)
plt.xlabel('Time of day')
plt.ylabel('Happiness Rating (out of 10)')
plt.title('My Self-Reported Happiness While Awake')
plt.show()

This will result in a labeled graph:

axis_labels

Now, you can try adding labels to plots of your own.


Subplots

Sometimes, we want to display two lines side-by-side, rather than in the same set of x- and y-axes. When we have multiple axes in the same picture, we call each set of axes a subplot. The picture or object that contains all of the subplots is called a figure.

We can have many different subplots in the same figure, and we can lay them out in many different ways. We can think of our layouts as having rows and columns of subplots. For instance, the following figure has six subplots split into 2 rows and 3 columns:

We can create subplots using .subplot().

The command plt.subplot() needs three arguments to be passed into it:

    The number of rows of subplots
    The number of columns of subplots
    The index of the subplot we want to create

For instance, the command plt.subplot(2, 3, 4) would create “Subplot 4” from the figure above.

Any plt.plot() that comes after plt.subplot() will create a line plot in the specified subplot. For instance:

# Data sets
x = [1, 2, 3, 4]
y = [1, 2, 3, 4]

# First Subplot
plt.subplot(1, 2, 1)
plt.plot(x, y, color='green')
plt.title('First Subplot')

# Second Subplot
plt.subplot(1, 2, 2)
plt.plot(x, y, color='steelblue')
plt.title('Second Subplot')

# Display both subplots
plt.show()

This would result in a figure with the two plots arranged like this:


Subplots Part II

Sometimes, when we’re putting multiple subplots together, some elements can overlap and make the figure unreadable:

overlapping

We can customize the spacing between our subplots to make sure that the figure we create is visible and easy to understand. To do this, we use the plt.subplots_adjust() command. .subplots_adjust() has some keyword arguments that can move your plots within the figure:

    left — the left-side margin, with a default of 0.125. You can increase this number to make room for a y-axis label
    right — the right-side margin, with a default of 0.9. You can increase this to make more room for the figure, or decrease it to make room for a legend
    bottom — the bottom margin, with a default of 0.1. You can increase this to make room for tick mark labels or an x-axis label
    top — the top margin, with a default of 0.9
    wspace — the horizontal space between adjacent subplots, with a default of 0.2
    hspace — the vertical space between adjacent subplots, with a default of 0.2

For example, if we were adding space to the bottom of a graph by changing the bottom margin to 0.2 (instead of the default of 0.1), we would use the command:

plt.subplots_adjust(bottom=0.2)

We can also use multiple keyword arguments, if we need to adjust multiple margins. For instance, we could adjust both the top and the hspace:

plt.subplots_adjust(top=0.95, hspace=0.25)

Let’s use wspace to fix the figure above:

# Left Plot
plt.subplot(1, 2, 1)
plt.plot([-2, -1, 0, 1, 2], [4, 1, 0, 1, 4])

# Right Plot
plt.subplot(1, 2, 2)
plt.plot([-2, -1, 0, 1, 2], [4, 1, 0, 1, 4])

# Subplot Adjust
plt.subplots_adjust(wspace=0.35)

plt.show()

This would give us figure with a better layout:

Legends

When we have multiple lines on a single graph we can label them by using the command plt.legend().

The legend method takes a list with the labels to display. So, for example, we can call:

plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16])
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64])
plt.legend(['parabola', 'cubic'])
plt.show()

which would display a legend on our graph, labeling each line: legend

plt.legend() can also take a keyword argument loc, which will position the legend on the figure.

These are the position values loc accepts:
Number Code 	String
0 	best
1 	upper right
2 	upper left
3 	lower left
4 	lower right
5 	right
6 	center left
7 	center right
8 	lower center
9 	upper center
10 	center
Note: If you decide not to set a value for loc, it will default to choosing the “best” location. 	

For, example, we can call plt.legend() and set loc to 6:

plt.legend(['parabola', 'cubic'], loc=6)
plt.show()

which would move the legend to the left side of the graph: legend_loc

Sometimes, it’s easier to label each line as we create it. If we want, we can use the keyword label inside of plt.plot(). If we choose to do this, we don’t pass any labels into plt.legend(). For example:

plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16],
         label="parabola")
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64],
         label="cubic")
plt.legend() # Still need this command!
plt.show()

This would display a legend that looks just like what we had before: legend


Modify Ticks

In all of our previous exercises, our commands have started with plt.. In order to modify tick marks, we’ll have to try something a little bit different.

Because our plots can have multiple subplots, we have to specify which one we want to modify. In order to do that, we call plt.subplot() in a different way.

ax = plt.subplot(1, 1, 1)

ax is an axes object, and it lets us modify the axes belonging to a specific subplot. Even if we only have one subplot, when we want to modify the ticks, we will need to start by calling either ax = plt.subplot(1, 1, 1) or ax = plt.subplot() in order to get our axes object.

Suppose we wanted to set our x-ticks to be at 1, 2, and 4. We would use the following code:

ax = plt.subplot()
plt.plot([0, 1, 2, 3, 4], [0, 1, 4, 9, 16])
plt.plot([0, 1, 2, 3, 4], [0, 1, 8, 27, 64])
ax.set_xticks([1, 2, 4])

Our result would look like this:

tick_marks

We can also modify the y-ticks by using ax.set_yticks().

When we change the x-ticks, their labels automatically change to match. But, if we want special labels (such as strings), we can use the command ax.set_xticklabels() or ax.set_yticklabels(). For example, we might want to have a y-axis with ticks at 0.1, 0.6, and 0.8, but label them 10%, 60%, and 80%, respectively. To do this, we use the following commands:

ax = plt.subplot()
plt.plot([1, 3, 3.5], [0.1, 0.6, 0.8], 'o')
ax.set_yticks([0.1, 0.6, 0.8])
ax.set_yticklabels(['10%', '60%', '80%'])

This would result in this y-axis labeling:

y_ticks

Now, let’s practice with tick marks.


Figures

When we’re making lots of plots, it’s easy to end up with lines that have been plotted and not displayed. If we’re not careful, these “forgotten” lines will show up in your new plots. In order to be sure that you don’t have any stray lines, you can use the command plt.close('all') to clear all existing plots before you plot a new one.

Previously, we learned how to put two sets of axes into the same figure. Sometimes, we would rather have two separate figures. We can use the command plt.figure() to create new figures and size them how we want. We can add the keyword figsize=(width, height) to set the size of the figure, in inches. We use parentheses (( and )) to pass in the width and height, which are separated by a comma (,).

To create a figure with a width of 4 inches, and height of 10 inches, we would use:

plt.figure(figsize=(4, 10))

It would look tall and skinny, like this:

tall_fig

Once we’ve created a figure, we might want to save it so that we can use it in a presentation or a website. We can use the command plt.savefig() to save out to many different file formats, such as png, svg, or pdf. After plotting, we can call plt.savefig('name_of_graph.png'):

# Figure 2
plt.figure(figsize=(4, 10)) 
plt.plot(x, parabola)
plt.savefig('tall_and_narrow.png')

This will save tall_and_narrow.png to our file system.


Now you’ve played around with several two-dimensional line plots in Matplotlib. You’ve seen how you can create simple, readable plots with few commands. You’ve also learned some commands to style and label your plots better. These are the concepts you’ve seen in Matplotlib so far:

    Creating a line graph from data
    Changing the appearance of the line
    Zooming in on different parts of the axis
    Putting labels on titles and axes
    Creating a more complex figure layout
    Adding legends to graphs
    Changing tick labels and positions
    Saving what you’ve made

Moving on, we’ll learn how to make different kinds of plots (beyond line graphs!) in Matplotlib and how to choose between those plots when displaying data.

Let’s do a final round of practice with all of the cool plotting concepts you’ve learned so far!

In summary:
import codecademylib
from matplotlib import pyplot as plt
x = range(1999, 2010)
y1 = [728.3, 753.9, 768.8, 780.1, 763.7, 788.5, 782, 787.2, 806.4, 806.2, 798.9]
y2 = [421, 465, 494, 538, 430, 530, 511, 600, 582, 605, 603]

plt.plot(x, y1, color="pink", marker="o")
plt.plot(x, y2, color="gray", marker="o")

plt.title("Two Lines on One Graph")
plt.ylabel("Incredible Y-axis")
plt.xlabel("Amazing X-axis")

plt.legend(["y1", "y2"], loc=4)

plt.show()

Simple Bar Chart

The plt.bar function allows you to create simple bar charts to compare multiple categories of data.

Some possible data that would be displayed with a bar chart:

    x-axis — famous buildings, y-axis — heights
    x-axis — different planets, y-axis — number of days in the year
    x-axis — programming languages, y-axis — lines of code written by you

You call plt.bar with two arguments:

    the x-values — a list of x-positions for each bar
    the y-values — a list of heights for each bar

In most cases, we will want our x-values to be a list that looks like [0, 1, 2, 3 ...] and has the same number of elements as our y-values list. We can create that list manually, but we can also use the following code:

heights = [88, 225, 365, 687, 4333, 10756, 30687, 60190, 90553]
x_values = range(len(heights))

The range function creates a list of consecutive integers (i.e., [0, 1, 2, 3, ...]). It needs an argument to tell it how many numbers should be in the list. For instance, range(5) would make a list with 5 numbers. We want our list to be as long as our bar heights (heights in this example). len(heights) tell us how many elements are in the list heights.

Here is an example of how to make a bar chart using plt.bar to compare the number of days in a year on the different planets:

days_in_year = [88, 225, 365, 687, 4333, 10756, 30687, 60190, 90553]
plt.bar(range(len(days_in_year)),
        days_in_year)
plt.show()

The result of this is:

planet_bar_chart

At this point, it’s hard to tell what this represents, because it’s unclearly labeled. We’ll fix that in later sections!

In the instructions below, we’ll use plt.bar to create a chart for a fake cafe called MatplotSip. We will be comparing the sales of different beverages on a given day.


imple Bar Chart II

When we create a bar chart, we want each bar to be meaningful and correspond to a category of data. In the drinks chart from the last exercise, we could see that sales were different for different drink items, but this wasn’t very helpful to us, since we didn’t know which bar corresponded to which drink.

In the previous lesson, we learned how to customize the tick marks on the x-axis in three steps:

    Create an axes object

    ax = plt.subplot()

    Set the x-tick positions using a list of numbers

    ax.set_xticks([0, 1, 2, 3, 4, 5, 6, 7, 8])

    Set the x-tick labels using a list of strings

    ax.set_xticklabels(['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune', 'Pluto'])

    If your labels are particularly long, you can use the rotation keyword to rotate your labels by a specified number of degrees:

    ax.set_xticklabels(['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune', 'Pluto'],
    rotation=30)

Note: We have to set the x-ticks before we set the x-labels because the default ticks won’t necessarily be one tick per bar, especially if we’re plotting a lot of bars. If we skip setting the x-ticks before the x-labels, we might end up with labels in the wrong place.

Remember from Lesson I that we can label the x-axis (plt.xlabel) and y-axis (plt.ylabel) as well. Now, our graph is much easier to understand: labeled_planet_chart

Let’s add the appropriate labels for the chart you made in the last exercise for the coffee shop, MatplotSip.


Side-By-Side Bars

We can use a bar chart to compare two sets of data with the same types of axis values. To do this, we plot two sets of bars next to each other, so that the values of each category can be compared. For example, here is a chart with side-by-side bars for the populations of the United States and China over the age of 65 (in percentages): population_bars

(data taken from World Bank)

Some examples of data that side-by-side bars could be useful for include:

    the populations of two countries over time
    prices for different foods at two different restaurants
    enrollments in different classes for males and females

In the graph above, there are 7 sets of bars, with 2 bars in each set. Each bar has a width of 0.8 (the default width for all bars in Matplotlib).

    If our first blue bar is at x=0, then we want the next blue bar to be at x=2, and the next to be at x=4, etc.
    Our first orange bar should be at x=0.8 (so that it is touching the blue bar), and the next orange bar should be at x=2.8, etc.

This is a lot of math, but we can make Python do it for us by copying and pasting this code:

# China Data (blue bars)
n = 1  # This is our first dataset (out of 2)
t = 2 # Number of datasets
d = 7 # Number of sets of bars
w = 0.8 # Width of each bar
x_values1 = [t*element + w*n for element
             in range(d)]

That just generated the first set of x-values. To generate the second set, paste the code again, but change n to 2, because this is the second dataset:

# US Data (orange bars)
n = 2  # This is our second dataset (out of 2)
t = 2 # Number of datasets
d = 7 # Number of sets of bars
w = 0.8 # Width of each bar
x_values2 = [t*element + w*n for element
             in range(d)]

Let’s examine our special code:

[t*element + w*n for element in range(d)]

This is called a list comprehension. It’s a special way of generating a list from a formula. You can learn more about it in this article. For making side-by-side bar graphs, you’ll never need to change this line; just paste it into your code and make sure to define n, t, d, and w correctly.

In the instructions below, we’ll experiment with side-by-side bars to compare different locations of the MatplotSip coffee empire.

EXAMPLE CODE
from matplotlib import pyplot as plt

drinks = ["cappuccino", "latte", "chai", "americano", "mocha", "espresso"]
sales1 =  [91, 76, 56, 66, 52, 27]
sales2 = [65, 82, 36, 68, 38, 40]

#Paste the x_values code here
n = 1  # This is our first dataset (out of 2)
t = 2 # Number of datasets
d = len(sales1) # Number of sets of bars
w = 0.8 # Width of each bar
store1_x = [t*element + w*n for element
             in range(d)]

plt.bar(store1_x, sales1)

n = 2  # This is our first dataset (out of 2)
t = 2 # Number of datasets
d = len(sales2) # Number of sets of bars
w = 0.8 # Width of each bar
store2_x = [t*element + w*n for element
             in range(d)]

plt.bar(store2_x, sales2)

plt.show()



Stacked Bars

If we want to compare two sets of data while preserving knowledge of the total between them, we can also stack the bars instead of putting them side by side. For instance, if someone was plotting the hours they’ve spent on entertaining themselves with video games and books in the past week, and wanted to also get a feel for total hours spent on entertainment, they could create a stacked bar chart:

entertainment

We do this by using the keyword bottom. The top set of bars will have bottom set to the heights of the other set of bars. So the first set of bars is plotted normally:

video_game_hours = [1, 2, 2, 1, 2]

plt.bar(range(len(video_game_hours)),
  video_game_hours) 

and the second set of bars has bottom specified:

book_hours = [2, 3, 4, 2, 1]

plt.bar(range(len(book_hours)),
  book_hours,
  bottom=video_game_hours)

This starts the book_hours bars at the heights of the video_game_hours bars. So, for example, on Monday the orange bar representing hours spent reading will start at a value of 1 instead of 0, because 1 hour was spent playing video games.

Let’s try this out with the MatplotSip data from the last exercise.



Error Bars

In the previous exercise, you learned to represent data as bars of different heights. Sometimes, we need to visually communicate some sort of uncertainty in the heights of those bars. Here are some examples:

    The average number of students in a 3rd grade classroom is 30, but some classes have as few as 18 and others have as many as 35 students.
    We measured that the weight of a certain fruit was 35g, but we know that our scale isn’t very precise, so the true weight of the fruit might be as much as 40g or as little as 30g.
    The average price of a soda is $1.00, but we also want to communicate that the standard deviation is $0.20.

To display error visually in a bar chart, we often use error bars to show where each bar could be, taking errors into account.

error_bars

Each of the black lines is called an error bar. The taller the bar is, the more uncertain we are about the height of the blue bar. The horizontal lines at the top and bottom are called caps. They make it easier to read the error bars.

If we wanted to show an error of +/- 2, we would add the keyword yerr=2 to our plt.bar command. To make the caps wide and easy to read, we would add the keyword capsize=10:

values = [10, 13, 11, 15, 20]
yerr = 2
plt.bar(range(len(values)), values, yerr=yerr, capsize=10)
plt.show()

If we want a different amount of error for each bar, we can make yerr equal to a list rather than a single number:

values = [10, 13, 11, 15, 20]
yerr = [1, 3, 0.5, 2, 4]
plt.bar(range(len(values)), values, yerr=yerr, capsize=10)
plt.show()

This code results in error bars of different sizes:

variable_error

Like the list of x-axis labels, Matplotlib reads this in the same order as the list of y-values. So, the first index of your error list should correspond to the first index of your y-values list, the second index of your error list should correspond to the second index of your y-values list, and so on.



Fill Between

We’ve learned how to display errors on bar charts using error bars. Let’s take a look at how we might do this in an aesthetically pleasing way on line graphs. In Matplotlib, we can use plt.fill_between to shade error. This function takes three arguments:

    x-values — this works just like the x-values of plt.plot
    lower-bound for y-values — sets the bottom of the shared area
    upper-bound for y-values — sets the top of the shared area

Generally, we use fill_between to create a shaded error region, and then plot the actual line over it. We can set the alpha keyword to a value between 0 and 1 in the fill_between call for transparency so that we can see the line underneath. Here is an example of how we would display data with an error of 2:

x_values = range(10)
y_values = [10, 12, 13, 13, 15, 19, 20, 22, 23, 29]
y_lower = [8, 10, 11, 11, 13, 17, 18, 20, 21, 27]
y_upper = [12, 14, 15, 15, 17, 21, 22, 24, 25, 31]

plt.fill_between(x_values, y_lower, y_upper, alpha=0.2) #this is the shaded error
plt.plot(x_values, y_values) #this is the line itself
plt.show()

This would give us a plot that looks like: fill_between

Having to calculate y_lower and y_upper by hand is time-consuming. If we try to just subtract 2 from y_values, we will get an error.

TypeError: unsupported operand type(s) for -: 'list' and 'int'

In order to correctly add or subtract from a list, we need to use list comprehension:

y_lower = [i - 2 for i in y_values]

This command looks at each element in y_values and calls the element its currently looking at i. For each new i, it subtracts 2. These opperations create a new list called y_lower.

If we wanted to add 2 to each element in y_values, we use this code:

y_upper = [i + 2 for i in y_values]

Pie Chart

If we want to display elements of a data set as proportions of a whole, we can use a pie chart.

Pie charts are helpful for displaying data like:

    Different ethnicities that make up a school district
    Different macronutrients (carbohydrates, fat, protein) that make up a meal
    Different responses to an online poll

In Matplotlib, you can make a pie chart with the command plt.pie, passing in the values you want to chart:

budget_data = [500, 1000, 750, 300, 100]

plt.pie(budget_data)
plt.show()

Which would create a chart like:

budget_skew

This looks weird and tilted. When we make pie charts in Matplotlib, we almost always want to set the axes to be equal to fix this issue. To do this, we use plt.axis('equal'), which results in a chart like this:





Pie Chart Labeling

We also want to be able to understand what each slice of the pie represents. To do this, we can either:

    use a legend to label each color, or
    put labels on the chart itself.

Method 1

budget_data = [500, 1000, 750, 300, 100]
budget_categories = ['marketing', 'payroll', 'engineering', 'design', 'misc']

plt.pie(budget_data)
plt.legend(budget_categories)

This puts the category names into a legend on the chart: pie_legend
Method 2

#option 2
plt.pie(budget_data, labels=budget_categories)

This puts the category names into labels next to each corresponding slice:

pie_labels

One other useful labeling tool for pie charts is adding the percentage of the total that each slice occupies. Matplotlib can add this automatically with the keyword autopct. We pass in string formatting instructions to format the labels how we want. Some common formats are:

    '%0.2f' — 2 decimal places, like 4.08
    '%0.2f%%' — 2 decimal places, but with a percent sign at the end, like 4.08%. You need two consecutive percent signs because the first one acts as an escape character, so that the second one gets displayed on the chart.
    '%d%%' — rounded to the nearest int and with a percent sign at the end, like 4%.

So, a full call to plt.pie might look like:

plt.pie(budget_data,
        labels=budget_categories,
        autopct='%0.1f%%')

and the resulting chart would look like: 




Histogram

Sometimes we want to get a feel for a large dataset with many samples beyond knowing just the basic metrics of mean, median, or standard deviation. To get more of an intuitive sense for a dataset, we can use a histogram to display all the values.

A histogram tells us how many values in a dataset fall between different sets of numbers (i.e., how many numbers fall between 0 and 10? Between 10 and 20? Between 20 and 30?). Each of these questions represents a bin, for instance, our first bin might be between 0 and 10.

All bins in a histogram are always the same size. The width of each bin is the distance between the minimum and maximum values of each bin. In our example, the width of each bin would be 10.

Each bin is represented by a different rectangle whose height is the number of elements from the dataset that fall within that bin.

Here is an example:

histogram

To make a histogram in Matplotlib, we use the command plt.hist. plt.hist finds the minimum and the maximum values in your dataset and creates 10 equally-spaced bins between those values.

The histogram above, for example, was created with the following code:

plt.hist(dataset) 
plt.show()

If we want more than 10 bins, we can use the keyword bins to set how many bins we want to divide the data into. The keyword range selects the minimum and maximum values to plot. For example, if we wanted to take our data from the last example and make a new histogram that just displayed the values from 66 to 69, divided into 40 bins (instead of 10), we could use this function call:

plt.hist(dataset, range=(66,69), bins=40)

which would result in a histogram that looks like this:

histogram_range

Histograms are best for showing the shape of a dataset. For example, you might see that values are close together, or skewed to one side. With this added intuition, we often discover other types of analysis we want to perform.




Multiple Histograms

If we want to compare two different distributions, we can put multiple histograms on the same plot. This could be useful, for example, in comparing the heights of a bunch of men and the heights of a bunch of women. However, it can be hard to read two histograms on top of each other. For example, in this histogram, we can’t see all of the blue plot, because it’s covered by the orange one:

overlap_hist

We have two ways we can solve a problem like this:

    use the keyword alpha, which can be a value between 0 and 1. This sets the transparency of the histogram. A value of 0 would make the bars entirely transparent. A value of 1 would make the bars completely opaque.

    plt.hist(a, range=(55, 75), bins=20, alpha=0.5)
    plt.hist(b, range=(55, 75), bins=20, alpha=0.5)

    This would make both histograms visible on the plot: alpha_histograms

    use the keyword histtype with the argument 'step' to draw just the outline of a histogram:

    plt.hist(a, range=(55, 75), bins=20, histtype='step')
    plt.hist(b, range=(55, 75), bins=20, histtype='step')

    which results in a chart like: step_histogram

Another problem we face is that our histograms might have different numbers of samples, making one much bigger than the other. We can see how this makes it difficult to compare qualitatively, by adding a dataset b with a much bigger size value:

a = normal(loc=64, scale=2, size=10000)
b = normal(loc=70, scale=2, size=100000)

plt.hist(a, range=(55, 75), bins=20)
plt.hist(b, range=(55, 75), bins=20)
plt.show()

The result is two histograms that are very difficult to compare: different_hist

To solve this, we can normalize our histograms using normed=True. This command divides the height of each column by a constant such that the total shaded area of the histogram sums to 1.

a = normal(loc=64, scale=2, size=10000)
b = normal(loc=70, scale=2, size=100000)

plt.hist(a, range=(55, 75), bins=20, alpha=0.5, normed=True)
plt.hist(b, range=(55, 75), bins=20, alpha=0.5, normed=True)
plt.show()

Now, we can more easily see the differences between the blue set and the orange set: 


Review

In helping MatplotSip visualize their data, you’ve learned a bunch of new plot types that you can use in Matplotlib. Congratulations on adding these new plotting abilities to your repertoire:

    How to compare categories of data with bar graphs
    Add error bars to graphs
    Use fill_between to display shaded error on line graphs
    Create stacked bar graphs for easier comparisons
    Create side-by-side bar graphs
    Create and format pie charts to compare proportional datasets
    Analyze frequency data using histograms, including multiple histograms on the same plot
    Normalize histograms

In the upcoming project, you will experiment with these different plot types and how to best use them to find patterns or trends in a new data set.


BAR CHART EXAMPLE ONE

from matplotlib import pyplot as plt

past_years_averages = [82, 84, 83, 86, 74, 84, 90]
years = [2000, 2001, 2002, 2003, 2004, 2005, 2006]
error = [1.5, 2.1, 1.2, 3.2, 2.3, 1.7, 2.4]

# Make your chart here
plt.figure(figsize=(10, 8))

plt.bar(range(len(past_years_averages)), past_years_averages, yerr=error, capsize=5)

ax = plt.subplot()

plt.title("Final Exam Averages")
plt.ylabel("Test Average")
plt.xlabel("Year")

# Set limits
ax.set_ylim([70, 95])
ax.set_xlim([-0.5, 6.5])

ax.set_xticks(range(len(years)))
ax.set_xticklabels(years)

plt.show()

plt.savefig('my_bar_chart.png')









import codecademylib
from matplotlib import pyplot as plt

unit_topics = ['Limits', 'Derivatives', 'Integrals', 'Diff Eq', 'Applications']
middle_school_a = [80, 85, 84, 83, 86]
middle_school_b = [73, 78, 77, 82, 86]

def create_x(t, w, n, d):
    return [t*x + w*n for x in range(d)]
school_a_x = create_x(2, 0.8, 1, len(middle_school_a))
school_b_x = create_x(2, 0.8, 2, len(middle_school_a))
# Make your chart here
plt.figure(figsize=(10,8))

ax = plt.subplot()

plt.bar(school_a_x, middle_school_a)
plt.bar(school_b_x, middle_school_b)

middle_x = [(a + b)/2.0 for a,b in zip(school_a_x,school_b_x)]

print(dir(plt))

ax.set_xticks(middle_x)
ax.set_xticklabels(unit_topics)

plt.legend(['Middle School A', 'Middle School B'])
plt.title('Test Averages on Different Units')

plt.xlabel('Unit')
plt.ylabel('Test Average')

plt.savefig('my_side_by_side.png')



plt.show()






















import codecademylib
from matplotlib import pyplot as plt
import numpy as np

unit_topics = ['Limits', 'Derivatives', 'Integrals', 'Diff Eq', 'Applications']
As = [6, 3, 4, 3, 5]
Bs = [8, 12, 8, 9, 10]
Cs = [13, 12, 15, 13, 14]
Ds = [2, 3, 3, 2, 1]
Fs = [1, 0, 0, 3, 0]

x = range(5)

c_bottom = np.add(As, Bs)
#create d_bottom and f_bottom here
d_bottom = np.add(c_bottom, Cs)
f_bottom = np.add(d_bottom, Ds)

#create your plot here
plt.figure(figsize=(10, 8))

# Create Base
plt.bar(range(len(As)), As)

# Create bottom bars
plt.bar(range(len(Bs)), Bs, bottom=As)
plt.bar(range(len(Cs)), Cs, bottom=c_bottom)
plt.bar(range(len(Ds)), Ds, bottom=d_bottom)
plt.bar(range(len(Fs)), Fs, bottom=f_bottom)

plt.title('Grade distribution')
plt.xlabel('Unit')
plt.ylabel('Number of Students')

ax = plt.subplot()

ax.set_xticks(range(len(unit_topics)))
ax.set_xticklabels(unit_topics)

plt.show()

plt.savefig('my_stacked_bar.png')






import codecademylib
from matplotlib import pyplot as plt

exam_scores1 = [62.58, 67.63, 81.37, 52.53, 62.98, 72.15, 59.05, 73.85, 97.24, 76.81, 89.34, 74.44, 68.52, 85.13, 90.75, 70.29, 75.62, 85.38, 77.82, 98.31, 79.08, 61.72, 71.33, 80.77, 80.31, 78.16, 61.15, 64.99, 72.67, 78.94]
exam_scores2 = [72.38, 71.28, 79.24, 83.86, 84.42, 79.38, 75.51, 76.63, 81.48,78.81,79.23,74.38,79.27,81.07,75.42,90.35,82.93,86.74,81.33,95.1,86.57,83.66,85.58,81.87,92.14,72.15,91.64,74.21,89.04,76.54,81.9,96.5,80.05,74.77,72.26,73.23,92.6,66.22,70.09,77.2]

# Make your plot here
plt.figure(figsize=(10, 8))

plt.hist(exam_scores1, bins=12, normed=True, histtype = 'step', linewidth=2)
plt.hist(exam_scores2, bins=12, normed=True, histtype = 'step', linewidth=2)

plt.legend(['1st Yr Teaching', '2nd Yr Teaching'])

plt.title('Final Exam Score Distribution')
plt.xlabel('Percentage')
plt.ylabel('Frequency')

plt.show()

plt.savefig('my_histogram.png')
















import codecademylib
from matplotlib import pyplot as plt

unit_topics = ['Limits', 'Derivatives', 'Integrals', 'Diff Eq', 'Applications']
num_hardest_reported = [1, 3, 10, 15, 1]

#Make your plot here
plt.figure(figsize=(10, 8))

plt.pie(num_hardest_reported, labels=unit_topics, autopct='%1d%%')
plt.axis('equal')

plt.title('Hardest Topics')

plt.show()

plt.savefig('my_pie_chart.png')














import codecademylib
from matplotlib import pyplot as plt

hours_reported =[3, 2.5, 2.75, 2.5, 2.75, 3.0, 3.5, 3.25, 3.25,  3.5, 3.5, 3.75, 3.75,4, 4.0, 3.75,  4.0, 4.25, 4.25, 4.5, 4.5, 5.0, 5.25, 5, 5.25, 5.5, 5.5, 5.75, 5.25, 4.75]
exam_scores = [52.53, 59.05, 61.15, 61.72, 62.58, 62.98, 64.99, 67.63, 68.52, 70.29, 71.33, 72.15, 72.67, 73.85, 74.44, 75.62, 76.81, 77.82, 78.16, 78.94, 79.08, 80.31, 80.77, 81.37, 85.13, 85.38, 89.34, 90.75, 97.24, 98.31]

plt.figure(figsize=(10,8))

# Create your hours_lower_bound and hours_upper_bound lists here 
hours_lower_bound = [error - (error * 0.2) for error in hours_reported]

hours_upper_bound = [error + (error * 0.2) for error in hours_reported]

# Make your graph here
plt.figure(figsize=(10, 8))

plt.plot(exam_scores, hours_reported, linewidth=2)
plt.fill_between(exam_scores, hours_lower_bound, hours_upper_bound, alpha=0.2)

plt.title('Time spent studying vs final exam scores')
plt.ylabel('Hours studying (self-reported)')
plt.xlabel('Score')

plt.show()

plt.savefig('my_line_graph.png')


\subsection{Seaborn}

Introduction to Seaborn

In this lesson, you’ll learn how to use Seaborn to create bar charts for statistical analysis.

Seaborn is a Python data visualization library that provides simple code to create elegant visualizations for statistical exploration and insight. Seaborn is based on Matplotlib, but improves on Matplotlib in several ways:

    Seaborn provides a more visually appealing plotting style and concise syntax.
    Seaborn natively understands Pandas DataFrames, making it easier to plot data directly from CSVs.
    Seaborn can easily summarize Pandas DataFrames with many rows of data into aggregated charts.

If you’re unfamiliar with Pandas, just know that Pandas is a data analysis library for Python that provides easy-to-use data structures and allows you to organize and manipulate datasets so they can be visualized. To fully leverage the power of Seaborn, it is best to prepare your data using Pandas.

Over the next few exercises, we will explain how Seaborn relates to Pandas and how we can transform massive datasets into easily understandable graphics.
Instructions
1.

The file script.py contains code to create a Seaborn visualization. Paste the following code at the very top of script.py to import Seaborn so that the code can run successfully:

import seaborn as sns

import warnings
warnings.filterwarnings('ignore')
import pandas as pd
from matplotlib import pyplot as plt

# Paste import here:
import seaborn as sns

df = pd.read_csv('survey.csv')
sns.barplot(x='Age Range', y='Response', hue='Gender', data=df)
plt.show()








Using Pandas For Seaborn

Throughout this lesson, you’ll use Seaborn to visualize a Pandas DataFrame.

DataFrames contain data structured into rows and columns. DataFrames look similar to other data tables you may be familiar with, but they are designed specifically to be used with Python.

You can create a DataFrame from a local CSV file (CSV files store data in a tabular format).

To create a DataFrame from a local CSV file you would use the syntax:

df = pd.read_csv('file_name.csv')

The code above creates a DataFrame saved to a variable named df. The data inside of the df DataFrame comes from the data in the local CSV file named file_name.csv.

Once you have prepared and organized a Pandas DataFrame with your chosen dataset, you are ready to plot with Seaborn!
Instructions
1.

In script.py you can see pd.read_csv() is used to ingest the data stored in a file named survey.csv. If you’d like, you can inspect the contents of survey.csv in the file system of your workspace. We will explain the context of survey.csv in more detail in the next exercise. For now, focus on the syntax used to create a DataFrame from a CSV file.

Inspect the DataFrame by printing the first 5 rows of df.


Plotting Bars with Seaborn

Take a look at the file called results.csv. You’ll plot that data soon, but before you plot it, take a minute to understand the context behind that data, which is based on a hypothetical situation we have created:

Suppose we are analyzing data from a survey: we asked 1,000 patients at a hospital how satisfied they were with their experience. Their response was measured on a scale of 1 - 10, with 1 being extremely unsatisfied, and 10 being extremely satisfied. We have summarized that data in a CSV file called results.csv.

To plot this data using Matplotlib, you would write the following:

df = pd.read_csv("results.csv")
ax = plt.subplot()
plt.bar(range(len(df)),
        df["Mean Satisfaction"])
ax.set_xticks(range(len(df)))
ax.set_xticklabels(df.Gender)
plt.xlabel("Gender")
plt.ylabel("Mean Satisfaction")

That's a lot of work for a simple bar chart! Seaborn gives us a much simpler option. With Seaborn, you can use the `sns.barplot()` command to do the same thing.

The Seaborn function sns.barplot(), takes at least three keyword arguments:

    data: a Pandas DataFrame that contains the data (in this example, data=df)
    x: a string that tells Seaborn which column in the DataFrame contains otheur x-labels (in this case, x="Gender")
    y: a string that tells Seaborn which column in the DataFrame contains the heights we want to plot for each bar (in this case y="Mean Satisfaction")

By default, Seaborn will aggregate and plot the mean of each category. In the next exercise you will learn more about aggregation and how Seaborn handles it.

Understanding Aggregates

Seaborn can also calculate aggregate statistics for large datasets. To understand why this is helpful, we must first understand what an aggregate is.

An aggregate statistic, or aggregate, is a single number used to describe a set of data. One example of an aggregate is the average, or mean of a data set. There are many other aggregate statistics as well.

Suppose we have a grade book with columns student, assignment_name, and grade, as shown below.
student 	assignment_name 	grade
Amy 	Assignment 1 	75
Amy 	Assignment 2 	82
Bob 	Assignment 1 	99
Bob 	Assignment 2 	90
Chris 	Assignment 1 	72
Chris 	Assignment 2 	66
… 	… 	…

To calculate a student’s current grade in the class, we need to aggregate the grade data by student. To do this, we’ll calculate the average of each student’s grades, resulting in the following data set:
student 	grade
Amy 	78.5
Bob 	94.5
Chris 	69
… 	…

On the other hand, we may be interested in understanding the relative difficulty of each assignment. In this case, we would aggregate by assignment, taking the average of all student’s scores on each assignment:
assignment_name 	grade
Assignment 1 	82
Assignment 2 	79.3
… 	…

In both of these cases, the function we used to aggregate our data was the average or mean, but there are many types of aggregate statistics including:

    Median
    Mode
    Standard Deviation

In Python, you can compute aggregates fairly quickly and easily using Numpy, a popular Python library for computing. You’ll use Numpy in this exercise to compute aggregates for a DataFrame.





import codecademylib3_seaborn
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np

gradebook = pd.read_csv("gradebook.csv")

print(gradebook)

assignment1 = gradebook[gradebook.assignment_name == 'Assignment 1']

print(assignment1)

asn1_median = np.median(assignment1['grade'])

print(asn1_median)


Plotting Aggregates

Recall our gradebook from the previous exercise:
student 	assignment_name 	grade
Amy 	Assignment 1 	75
Amy 	Assignment 2 	82
Bob 	Assignment 1 	99
Bob 	Assignment 2 	90
Chris 	Assignment 1 	72
Chris 	Assignment 2 	66
… 	… 	…

Suppose this data is stored in a Pandas DataFrame called df.

The same Seaborn command that you previously learned (sns.barplot()) will plot this data in a bar plot and automatically aggregate the data:

sns.barplot(data=df, x="student", y="grade")

In the example above, Seaborn will aggregate grades by student, and plot the average grade for each student.


import codecademylib3_seaborn
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

gradebook = pd.read_csv("gradebook.csv")

sns.barplot(data=gradebook, x='assignment_name', y='grade')

plt.show()


Modifying Error Bars

By default, Seaborn will place error bars on each bar when you use the barplot() function.

Error bars are the small lines that extend above and below the top of each bar. Errors bars visually indicate the range of values that might be expected for that bar.

For example, in our assignment average example, an error bar might indicate what grade we expect an average student to receive on this assignment.

There are several different calculations that are commonly used to determine error bars.

By default, Seaborn uses something called a bootstrapped confidence interval. Roughly speaking, this interval means that “based on this data, 95% of similar situations would have an outcome within this range”.

In our gradebook example, the confidence interval for the assignments means “if we gave this assignment to many, many students, we’re confident that the mean score on the assignment would be within the range represented by the error bar”.

The confidence interval is a nice error bar measurement because it is defined for different types of aggregate functions, such as medians and mode, in addition to means.

If you’re calculating a mean and would prefer to use standard deviation for your error bars, you can pass in the keyword argument ci="sd" to sns.barplot() which will represent one standard deviation. It would look like this:

sns.barplot(data=gradebook, x="name", y="grade", ci="sd")



Calculating Different Aggregates

In most cases, we’ll want to plot the mean of our data, but sometimes, we’ll want something different:

    If our data has many outliers, we may want to plot the median.
    If our data is categorical, we might want to count how many times each category appears (such as in the case of survey responses).

Seaborn is flexible and can calculate any aggregate you want. To do so, you’ll need to use the keyword argument estimator, which accepts any function that works on a list.

For example, to calculate the median, you can pass in np.median to the estimator keyword:

sns.barplot(data=df,
  x="x-values",
  y="y-values",
  estimator=np.median)

Consider the data in results.csv. To calculate the number of times a particular value appears in the Response column , we pass in len:

sns.barplot(data=df,
  x="Patient ID",
  y="Response",
  estimator=len)


Aggregating by Multiple Columns

Sometimes we’ll want to aggregate our data by multiple columns to visualize nested categorical variables.

For example, consider our hospital survey data. The mean satisfaction seems to depend on Gender, but it might also depend on another column: Age Range.

We can compare both the Gender and Age Range factors at once by using the keyword hue.

sns.barplot(data=df,
            x="Gender",
            y="Response",
            hue="Age Range")

The hue parameter adds a nested categorical variable to the plot.
*Visualizing survey results by gender with age range nested*.

Notice that we keep the same x-labels, but we now have different color bars representing each Age Range. We can compare two bars of the same color to see how patients with the same Age Range, but different Gender rated the survey.


import codecademylib3_seaborn
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

df = pd.read_csv("survey.csv")

sns.barplot(data=df, x='Age Range', y='Response', hue='Gender')

plt.show()



Review

In this lesson you learned how to extend Matplotlib with Seaborn to create meaningful visualizations from data in DataFrames.

You’ve also learned how Seaborn creates aggregated charts and how to change the way aggregates and error bars are calculated.

Finally, you learned how to aggregate by multiple columns, and how the hue parameter adds a nested categorical variable to a visualization.
To review the seaborn workflow:
1. Ingest data from a CSV file to Pandas DataFrame.

df = pd.read_csv('file_name.csv')

2. Set sns.barplot() with desired values for x, y, and set data equal to your DataFrame.

sns.barplot(data=df, x='X-Values', y='Y-Values')

3. Set desired values for estimator and hue parameters.

sns.barplot(data=df, x='X-Values', y='Y-Values', estimator=len, hue='Value')

4. Render the plot using plt.show().

plt.show()



\subsection{Seaborn Distributions}

Introduction

In this lesson, we will explore how to use Seaborn to graph multiple statistical distributions, including box plots and violin plots.

Seaborn is optimized to work with large datasets — from its ability to natively interact with Pandas DataFrames, to automatically calculating and plotting aggregates. One of the most powerful aspects of Seaborn is its ability to visualize and compare distributions. Distributions provide us with more information about our data — how spread out it is, its range, etc.

Calculating and graphing distributions is integral to analyzing massive amounts of data. We’ll look at how Seaborn allows us to move beyond the traditional distribution graphs to plots that enable us to communicate important statistical information.
Instructions

To your right, you’ll find a Jupyter notebook with some example Seaborn charts. We won’t be using Jupyter notebooks in this lesson, but they’re a great way of combining text, code, and visualization. You can find more about them on the Jupyter website.

Take a moment to look at the graphs to the right. What do you notice about each of these charts? What kind of statistical information could they be trying to convey?

When you’re ready, continue to the first exercise.




Plotting Distributions with Seaborn

Seaborn's strength is in visualizing statistical calculations. Seaborn includes several plots that allow you to graph univariate distribution, including KDE plots, box plots, and violin plots. Explore the Jupyter notebook below to get an understanding of how each plot works.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

First, we'll read in three datasets. In order to plot them in Seaborn, we'll combine them using NumPy's .concatenate() function into a Pandas DataFrame.

n = 500
dataset1 = np.genfromtxt("dataset1.csv", delimiter=",")
dataset2 = np.genfromtxt("dataset2.csv", delimiter=",")
dataset3 = np.genfromtxt("dataset3.csv", delimiter=",")


df = pd.DataFrame({
    "label": ["set_one"] * n + ["set_two"] * n + ["set_three"] * n,
    "value": np.concatenate([dataset1, dataset2, dataset3])
})

sns.set()

First, let's plot each dataset as bar charts.

sns.barplot(data=df, x='label', y='value')
plt.show()

We can use barplots to find out information about the mean - but it doesn't give us a sense of how spread out the data is in each set. To find out more about the distribution, we can use a KDE plot.

sns.kdeplot(dataset1, shade=True, label="dataset1")
sns.kdeplot(dataset2, shade=True, label="dataset2")
sns.kdeplot(dataset3, shade=True, label="dataset3")

plt.legend()
plt.show()

A KDE plot will give us more information, but it's pretty difficult to read this plot.

sns.boxplot(data=df, x='label', y='value')
plt.show()

A box plot, on the other hand, makes it easier for us to compare distributions. It also gives us other information, like the interquartile range and any outliers. However, we lose the ability to determine the shape of the data.

sns.violinplot(data=df, x="label", y="value")
plt.show()

A violin plot brings together shape of the KDE plot with additional information that a box plot provides. It's understandable why many people like this plot!



Bar Charts Hide Information

Before we dive into these new charts, we need to understand why we’d want to use them. To best illustrate this idea, we need to revisit bar charts.

We previously learned that Seaborn can quickly aggregate data to plot bar charts using the mean.

Here is a bar chart that uses three different randomly generated sets of data:

sns.barplot(data=df, x="label", y="value")
plt.show()

alt

These three datasets look identical! As far as we can tell, they each have the same mean and similar confidence intervals.

We can get a lot of information from these bar charts, but we can’t get everything. For example, what are the minimum and maximum values of these datasets? How spread out is this data?

While we may not see this information in our bar chart, these differences might be significant and worth understanding better.


import codecademylib3_seaborn
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

# Take in the data from the CSVs as NumPy arrays:
set_one = np.genfromtxt("dataset1.csv", delimiter=",")
set_two = np.genfromtxt("dataset2.csv", delimiter=",")
set_three = np.genfromtxt("dataset3.csv", delimiter=",")
set_four = np.genfromtxt("dataset4.csv", delimiter=",")

# Creating a Pandas DataFrame:
n=500
df = pd.DataFrame({
    "label": ["set_one"] * n + ["set_two"] * n + ["set_three"] * n + ["set_four"] * n,
    "value": np.concatenate([set_one, set_two, set_three, set_four])
})

# Setting styles:
sns.set_style("darkgrid")
sns.set_palette("pastel")

# Add your code below:
sns.barplot(data=df, x="label", y="value")

plt.show()

Learn Seaborn: Distributions
KDE Plots, Part I

Bar plots can tell us what the mean of our dataset is, but they don’t give us any hints as to the distribution of the dataset values. For all we know, the data could be clustered around the mean or spread out evenly across the entire range.

To find out more about each of these datasets, we’ll need to examine their distributions. A common way of doing so is by plotting the data as a histogram, but histograms have their drawback as well.

Seaborn offers another option for graphing distributions: KDE Plots.

KDE stands for Kernel Density Estimator. A KDE plot gives us the sense of a univariate as a curve. A univariate dataset only has one variable and is also referred to as being one-dimensional, as opposed to bivariate or two-dimensional datasets which have two variables.

KDE plots are preferable to histograms because depending on how you group the data into bins and the width of the bins, you can draw wildly different conclusions about the shape of the data. Using a KDE plot can mitigate these issues, because they smooth the datasets, allow us to generalize over the shape of our data, and aren’t beholden to specific data points.


KDE Plots, Part II

To plot a KDE in Seaborn, we use the method sns.kdeplot().

A KDE plot takes the following arguments:

    data - the univariate dataset being visualized, like a Pandas DataFrame, Python list, or NumPy array
    shade - a boolean that determines whether or not the space underneath the curve is shaded

Let’s examine the KDE plots of our three datasets:

sns.kdeplot(dataset1, shade=True)
sns.kdeplot(dataset2, shade=True)
sns.kdeplot(dataset3, shade=True)
plt.legend()
plt.show()

alt

Notice that when using a KDE we need to plot each of the original datasets separately, rather than using a combined dataframe like we did with the bar plot.

It looks like there are some big differences between the three datasets:

    Dataset 1 is skewed left
    Dataset 2 is normally distributed
    Dataset 3 is bimodal (it has two peaks)

So although all three datasets have roughly the same mean, the shapes of the KDE plots demonstrate the differences in how the values are distributed.



import codecademylib3_seaborn
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

# Take in the data from the CSVs as NumPy arrays:
set_one = np.genfromtxt("dataset1.csv", delimiter=",")
set_two = np.genfromtxt("dataset2.csv", delimiter=",")
set_three = np.genfromtxt("dataset3.csv", delimiter=",")
set_four = np.genfromtxt("dataset4.csv", delimiter=",")

# Creating a Pandas DataFrame:
n=500
df = pd.DataFrame({
    "label": ["set_one"] * n + ["set_two"] * n + ["set_three"] * n + ["set_four"] * n,
    "value": np.concatenate([set_one, set_two, set_three, set_four])
})

# Setting styles:
sns.set_style("darkgrid")
sns.set_palette("pastel")

# Add your code below:
sns.kdeplot(set_one, shade=True)
sns.kdeplot(set_two, shade=True)
sns.kdeplot(set_three, shade=True)
sns.kdeplot(set_four, shade=True)

plt.show()


Box Plots, Part I

While a KDE plot can tell us about the shape of the data, it’s cumbersome to compare multiple KDE plots at once. They also can’t tell us other statistical information, like the values of outliers.

The box plot (also known as a box-and-whisker plot) can tell us about how our dataset is distributed, like a KDE plot. But it shows us the range of our dataset, gives us an idea about where a significant portion of our data lies, and whether or not any outliers are present.

Let’s examine how we interpret a box plot:

    The box represents the interquartile range
    The line in the middle of the box is the median
    The end lines are the first and third quartiles
    The diamonds show outliers



Box Plots, Part II

One advantage of the box plot over the KDE plot is that in Seaborn, it is easy to plot multiples and compare distributions.

Let’s look again at our three datasets, and how they look plotted as box plots:

sns.boxplot(data=df, x='label', y='value')
plt.show()

alt

The box plot does a good job of showing certain differences, the different between Dataset 1 and Dataset 2; however, it does not show that Dataset 3 is bimodal.

To plot a box plot in Seaborn, we use the method sns.boxplot().

A box plot takes the following arguments:

    data - the dataset we’re plotting, like a DataFrame, list, or an array
    x - a one-dimensional set of values, like a Series, list, or array
    y - a second set of one-dimensional data

If you use a Pandas Series for the x and y values, the Series will also generate the axis labels. For example, if you use the value Series as your y value data, Seaborn will automatically apply that name as the y-axis label.


import codecademylib3_seaborn
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

# Take in the data from the CSVs as NumPy arrays:
set_one = np.genfromtxt("dataset1.csv", delimiter=",")
set_two = np.genfromtxt("dataset2.csv", delimiter=",")
set_three = np.genfromtxt("dataset3.csv", delimiter=",")
set_four = np.genfromtxt("dataset4.csv", delimiter=",")

# Creating a Pandas DataFrame:
n=500
df = pd.DataFrame({
    "label": ["set_one"] * n + ["set_two"] * n + ["set_three"] * n + ["set_four"] * n,
    "value": np.concatenate([set_one, set_two, set_three, set_four])
})

# Setting styles:
sns.set_style("darkgrid")
sns.set_palette("pastel")

# Add your code below:
sns.boxplot(data=df, x="label", y="value")

plt.show()


Violin Plots, Part I

As we saw in the previous exercises, while it’s possible to plot multiple histograms, it is not a great option for comparing distributions. Seaborn gives us another option for comparing distributions - a violin plot. Violin plots provide more information than box plots because instead of mapping each individual data point, we get an estimation of the dataset thanks to the KDE.

Violin plots are less familiar and trickier to read, so let’s break down the different parts:

    There are two KDE plots that are symmetrical along the center line.
    A white dot represents the median.
    The thick black line in the center of each violin represents the interquartile range.
    The lines that extend from the center are the confidence intervals - just as we saw on the bar plots, a violin plot also displays the 95% confidence interval.

Violin Plots, Part II

Violin Plots are a powerful graphing tool that allows you to compare multiple distributions at once.

Let’s look at how our original three data sets look like as violin plots:

sns.violinplot(data=df, x="label", y="value")
plt.show()

alt

As we can see, violin plots allow us to graph and compare multiple distributions. It also retains the shape of the distributions, so we can easily tell that Dataset 1 is skewed left and that Dataset 3 is bimodal.

To plot a violin plot in Seaborn, use the method sns.violinplot().

There are several options for passing in relevant data to the x and y parameters:

    data - the dataset that we’re plotting, such as a list, DataFrame, or array
    x, y, and hue - a one-dimensional set of data, such as a Series, list, or array
    any of the parameters to the function sns.boxplot()



import codecademylib3_seaborn
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

# Take in the data from the CSVs as NumPy arrays:
set_one = np.genfromtxt("dataset1.csv", delimiter=",")
set_two = np.genfromtxt("dataset2.csv", delimiter=",")
set_three = np.genfromtxt("dataset3.csv", delimiter=",")
set_four = np.genfromtxt("dataset4.csv", delimiter=",")

# Creating a Pandas DataFrame:
n=500
df = pd.DataFrame({
    "label": ["set_one"] * n + ["set_two"] * n + ["set_three"] * n + ["set_four"] * n,
    "value": np.concatenate([set_one, set_two, set_three, set_four])
})

# Setting styles:
sns.set_style("darkgrid")
sns.set_palette("pastel")

# Add your code below:
sns.violinplot(data=df, x="label", y="value")

plt.show()


Review

In this lesson, we examined how Seaborn has several plots that can visualize distributions. While bar plots can display basic aggregates, KDE plots, dist plots, box plots, and violin plots can show us distributions and other information.

    KDE plot - Kernel density estimator; shows a smoothed version of dataset. Use sns.kdeplot().
    Box plot - A classic statistical model that shows the median, interquartile range, and outliers. Use sns.boxplot().
    Violin plot - A combination of a KDE and a box plot. Good for showing multiple distributions at a time. Use sns.violinplot().

Standard deviation and bootstrapped confidence intervals are two measurements that can be used for:


Error bars

To change how error bars are calculated you can use the keyword ci. ci="sd" for example would set the error bars to be calculated using standard deviation.


\subsection{Styling Seaborn}
Seaborn Styling, Part 1: Figure Style and Scale

Learn how to customize your figures and scale plots for different presentation settings.
Introduction

When creating a data visualization, your goal is to communicate the insights found in the data. While visualizing communicates important information, styling will influence how your audience understands what you’re trying to convey.

After you have formatted and visualized your data, the third and last step of data visualization is styling. Styling is the process of customizing the overall look of your visualization, or figure. Making intentional decisions about the details of the visualization will increase their impact and set your work apart.

In this article, we’ll look at how to do the following techniques in Seaborn:

    customize the overall look of your figure, using background colors, grids, spines, and ticks
    scale plots for different contexts, such as presentations and reports

Customizing the Overall Look of Your Figure

Seaborn enables you to change the presentation of your figures by changing the style of elements like the background color, grids, and spines. When deciding how to style your figures, you should take into consideration your audience and the context. Is your visualization part of a report and needs to convey specific information? Or is it part of a presentation? Or is your visualization meant as its own stand-alone, with no narrator in front of it, and no other visualizations to compare it to?

In this section, we’ll explore three main aspects of customizing figures in Seaborn - background color, grids, and spines - and how these elements can change the look and meaning of your visualizations.
Built-in Themes

Seaborn has five built-in themes to style its plots: darkgrid, whitegrid, dark, white, and ticks. Seaborn defaults to using the darkgrid theme for its plots, but you can change this styling to better suit your presentation needs.

To use any of the preset themes pass the name of it to sns.set_style().

sns.set_style("darkgrid")
sns.stripplot(x="day", y="total_bill", data=tips)

image1

We’ll explore the rest of the themes in the examples below.
Background Color

When thinking about the look of your visualization, one thing to consider is the background color of your plot. The higher the contrast between the color palette of your plot and your figure background, the more legible your data visualization will be. Fun fact: dark blue on white is actually more legible than black on white!

The dark background themes provide a nice change from the Matplotlib styling norms, but doesn’t have as much contrast:

sns.set_style("dark")
sns.stripplot(x="day", y="total_bill", data=tips)

image2

The white and tick themes will allow the colors of your dataset to show more visibly and provides higher contrast so your plots are more legible:

sns.set_style("ticks")
sns.stripplot(x="day", y="total_bill", data=tips)

image3
Grids

In addition to being able to define the background color of your figure, you can also choose whether or not to include a grid. Remember that the default theme includes a grid.

It’s a good choice to use a grid when you want your audience to be able to draw their own conclusions about data. A grid allows the audience to read your chart and get specific information about certain values. Research papers and reports are a good example of when you would want to include a grid.

sns.set_style("whitegrid")
sns.stripplot(x="day", y="total_bill", data=tips)

image4

There are also instances where it would make more sense to not use a grid. If you’re delivering a presentation, simplifying your charts in order to draw attention to the important visual details may mean taking out the grid. If you’re interested in making more specific design choices, then leaving out the grids might be part of that aesthetic decision.

sns.set_style("white")
sns.stripplot(x="day", y="total_bill", data=tips)

image5

In this case, a blank background would allow your plot to shine.
Despine

In addition to changing the color background, you can also define the usage of spines. Spines are the borders of the figure that contain the visualization. By default, an image has four spines.

You may want to remove some or all of the spines for various reasons. A figure with the left and bottom spines resembles traditional graphs. You can automatically take away the top and right spines using the sns.despine()function. Note: this function must be called after you have called your plot.

sns.set_style("white")
sns.stripplot(x="day", y="total_bill", data=tips)
sns.despine()

image6

Not including any spines at all may be an aesthetic decision. You can also specify how many spines you want to include by calling despine() and passing in the spines you want to get rid of, such as: left, bottom, top, right.

sns.set_style("whitegrid")
sns.stripplot(x="day", y="total_bill", data=tips)
sns.despine(left=True, bottom=True)

image7
Scaling Figure Styles for Different Mediums

Matplotlib allows you to generate powerful plots, but styling those plots for different presentation purposes is difficult. Seaborn makes it easy to produce the same plots in a variety of different visual formats so you can customize the presentation of your data for the appropriate context, whether it be a research paper or a conference poster.

You can set the visual format, or context, using sns.set_context()

Within the usage of sns.set_context(), there are three levels of complexity:

    Pass in one parameter that adjusts the scale of the plot
    Pass in two parameters - one for the scale and the other for the font size
    Pass in three parameters - including the previous two, as well as the rc with the style parameter that you want to override

Scaling Plots

Seaborn has four presets which set the size of the plot and allow you to customize your figure depending on how it will be presented.

In order of relative size they are: paper, notebook, talk, and poster. The notebook style is the default.

sns.set_style("ticks")

# Smallest context:
sns.set_context("paper")
sns.stripplot(x="day", y="total_bill", data=tips)

image8

sns.set_style("ticks")

# Largest Context:
sns.set_context("poster")
sns.stripplot(x="day", y="total_bill", data=tips)

image9
Scaling Fonts and Line Widths

You are also able to change the size of the text using the font_scale parameter for sns.set_context()

You may want to also change the line width so it matches. We do this with the rc parameter, which we’ll explain in detail below.

# Set font scale and reduce grid line width to match
sns.set_context("poster", font_scale = .5, rc={"grid.linewidth": 0.6})
sns.stripplot(x="day", y="total_bill", data=tips)

image10

While you’re able to change these parameters, you should keep in mind that it’s not always useful to make certain changes. Notice in this example that we’ve changed the line width, but because of it’s relative size to the plot, it distracts from the actual plotted data.

# Set font scale and increase grid line width to match
sns.set_context("poster", font_scale = 1, rc={"grid.linewidth": 5})
sns.stripplot(x="day", y="total_bill", data=tips)

image11
The RC Parameter

As we mentioned above, if you want to override any of these standards, you can use sns.set_context and pass in the parameter rc to target and reset the value of an individual parameter in a dictionary. rc stands for the phrase ‘run command’ - essentially, configurations which will execute when you run your code.

sns.set_style("ticks")
sns.set_context("poster")
sns.stripplot(x="day", y="total_bill", data=tips)
sns.plotting_context()

Returns:

{'axes.labelsize': 17.6,
 'axes.titlesize': 19.200000000000003,
 'font.size': 19.200000000000003,
 'grid.linewidth': 1.6,
 'legend.fontsize': 16.0,
 'lines.linewidth': 2.8000000000000003,
 'lines.markeredgewidth': 0.0,
 'lines.markersize': 11.200000000000001,
 'patch.linewidth': 0.48,
 'xtick.labelsize': 16.0,
 'xtick.major.pad': 11.200000000000001,
 'xtick.major.width': 1.6,
 'xtick.minor.width': 0.8,
 'ytick.labelsize': 16.0,
 'ytick.major.pad': 11.200000000000001,
 'ytick.major.width': 1.6,
 'ytick.minor.width': 0.8}

Conclusion

As you can see, Seaborn offers a lot of opportunities to customize your plots and have them show a distinct style. The color of your background, background style such as lines and ticks, and the size of your font all play a role in improving legibility and aesthetics.


\subsection{Styling Seaborn Colour}
Seaborn Styling, Part 2: Color

Learn how to work with color in Seaborn and choose appropriate color palettes for your datasets.
Introduction

When creating a data visualization, your goal is to communicate the insights found in the data. While visualizing communicates important information, styling will influence how your audience understands what you’re trying to convey.

After you have formatted and visualized your data, the third and last step of data visualization is styling. Styling is the process of customizing the overall look of your visualization, or figure. Making intentional decisions about the details of the visualization will increase their impact and set your work apart.

In this article, we’ll look at how we can effectively use color to convey meaning. We’ll cover:

    How to set a palette
    Seaborn default and built-in color palettes
    Color Brewer Palettes
    Selecting palettes for your dataset

Commands for Working with Palettes

You can build color palettes using the function sns.color_palette(). This function can take any of the Seaborn built-in palettes (see below). You can also build your own palettes by passing in a list of colors in any valid Matplotlib format, including RGB tuples, hex color codes, or HTML color names.

If you want to quickly see what a palette looks like, use the function sns.palplot() to plot a palette as an array of colors:

# Save a palette to a variable:
palette = sns.color_palette("bright")

# Use palplot and pass in the variable:
sns.palplot(palette)

image1

To select and set a palette in Seaborn, use the command sns.set_palette() and pass in the name of the palette that you would like to use:

# Set the palette using the name of a palette:
sns.set_palette("Paired")

# Plot a chart:
sns.stripplot(x="day", y="total_bill", data=tips)

image2
Seaborn Default Color Palette

If you do not pass in a color palette to sns.color_palette() or sns.set_palette(), Seaborn will use a default set of colors. These defaults improve upon the Matplotlib default color palettes and are one significant reason why people choose to use Seaborn for their data visualizations. Here’s a comparison of the two default palettes:

image3

image4

Seaborn also allows you to style Matplotlib plots. So even if you’re using a plot that only exists in Matplotlib, such as a histogram, you can do so using Seaborn defaults.

To do so, call the sns.set() function before your plot:

# Call the sns.set() function 
sns.set()
for col in 'xy':
  plt.hist(data[col], normed=True, alpha=0.5)

image5

Not only does this function allow you the ability to use Seaborn default colors, but also any of Seaborn’s other styling techniques.

Seaborn has six variations of its default color palette: deep, muted, pastel, bright, dark, and colorblind.

image6

To use one of these palettes, pass the name into sns.set_palette():

# Set the palette to the "pastel" default palette:
sns.set_palette("pastel")

# plot using the "pastel" palette
sns.stripplot(x="day", y="total_bill", data=tips)

image7
Using Color Brewer Palettes

In addition to the default palette and its variations, Seaborn also allows the use of Color Brewer palettes. Color Brewer is the name of a set of color palettes inspired by the research of cartographer Cindy Brewer. The color palettes are specifically chosen to be easy to interpret when used to represent ordered categories. They are also colorblind accessible, as each color differs from its neighbors in lightness or tint.

To use, pass the name of any Color Brewer palette directly to any of the color functions:

custom_palette = sns.color_palette("Paired", 9)
sns.palplot(custom_palette)

image8

Here is a list of the the Color Brewer palettes, with their names for easy reference:

image9

Check out http://colorbrewer2.org for more information about color palette configuration options.
Selecting Color Palettes for Your Dataset
Qualitative Palettes for Categorical Datasets

When using a dataset that uses distinct but non-ordered categories, it’s good to use qualitative palettes. Qualitative palettes are sets of distinct colors which make it easy to distinguish the categories when plotted but don’t imply any particular ordering or meaning.

An example of categorical data is breed of dog. Each of these values, such as Border Collie or Poodle, are distinct from each other but there isn’t any inherent order to these categories.

Here’s an example of a qualitative Color Brewer palette:

qualitative_colors = sns.color_palette("Set3", 10)
sns.palplot(qualitative_colors)

image10
Sequential Palettes

Just as the name implies, sequential palettes are a set of colors that move sequentially from a lighter to a darker color. Sequential color palettes are appropriate when a variable exists as ordered categories, such as grade in school, or as continuous values that can be put into groups, such as yearly income. Because the darkest colors will attract the most visual attention, sequential palettes are most useful when only high values need to be emphasized.

Here’s an example of a sequential Color Brewer palette:

sequential_colors = sns.color_palette("RdPu", 10)
sns.palplot(sequential_colors)

image11
Diverging Palettes

Diverging palettes are best suited for datasets where both the low and high values might be of equal interest, such as hot and cold temperatures.

In the example below, both ends of the spectrum — fire red and deep blue — are likely to attract attention.

diverging_colors = sns.color_palette("RdBu", 10)
sns.palplot(diverging_colors)

image12

Here is a quick diagram that depicts each of the palette types:

image13
Credit: Michael Waskom
Summary

The ability to use easily choose different color palettes is one of the important affordances of styling your plots with Seaborn. Seaborn gives you a range of built-in plots to choose from: whether it’s variations on the defaults or access to all of the Color Brewer palettes. It’s easy to choose a palette that is well suited to your dataset, thanks to Color Brewer, as it supports palettes for qualitative, sequential, and diverging datasets.

For more on using color in Seaborn, check out their documentation.


Kaggle is a platform for data science competitions that hosts many datasets online.

https://www.kaggle.com/abecklas/fifa-world-cup



import pandas as pd

restaurants = pd.read_csv('restaurants.csv')

print(restaurants.head())

cuisine_options_count = restaurants.cuisine.nunique()

cuisine_counts = restaurants.groupby('cuisine').id.count().reset_index()

print(cuisine_counts)












from matplotlib import pyplot as plt
import pandas as pd

restaurants = pd.read_csv('restaurants.csv')

cuisine_counts = restaurants.groupby('cuisine')\
                            .name.count()\
                            .reset_index()


print(cuisine_counts.head())

cuisines = cuisine_counts["cuisine"]
counts = cuisine_counts["name"]

plt.pie(counts, labels=cuisines, autopct='%d%%')
plt.axis('equal')
plt.title('Count of Restaurants')

plt.show()







from matplotlib import pyplot as plt
import pandas as pd
from string import split

orders = pd.read_csv('orders.csv')

print(orders.head())

#ALTERNATIVELY
#orders['month'] = orders.date.apply(lambda x: x.split('-')[0])
orders['month'] = orders.date.str.split('-').str[0]

avg_order = orders.groupby('month').price.mean().reset_index()

std_order = orders.groupby('month').price.std().reset_index()



















from matplotlib import pyplot as plt
import pandas as pd

orders = pd.read_csv('orders.csv')

orders['month'] = orders.date.apply(lambda x: x.split('-')[0])

avg_order = orders.groupby('month').price.mean().reset_index()

std_order = orders.groupby('month').price.std().reset_index()

bar_heights = avg_order.price
bar_errors = std_order.price


ax = plt.subplot()
plt.bar(range(len(avg_order)), bar_heights, yerr=bar_errors, capsize=5)

plt.ylabel("Average Order")
plt.title("Average Order per Month")

ax.set_xticks(range(len(avg_order)))
ax.set_xticklabels(['April','May','June','July','August', 'September'])

plt.show()











from matplotlib import pyplot as plt
import pandas as pd

orders = pd.read_csv('orders.csv')

customer_amount = orders.groupby("customer_id").price.sum().reset_index()

print(customer_amount.head())

plt.hist(customer_amount.price, bins=40, range=(0,200))

plt.xlabel("Total Spent")
plt.ylabel("Number of Customers")
plt.title("Total amount spent per customer")

plt.show()


\end{document}